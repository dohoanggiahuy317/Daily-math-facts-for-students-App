Differential of a function - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Donate Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Create account Log in Personal tools Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 History and usage 2 Definition 3 Differentials in several variables Toggle Differentials in several variables subsection 3.

1 Application of the total differential to error estimation 4 Higher-order differentials 5 Properties 6 General formulation 7 Other approaches 8 Examples and applications 9 Notes 10 See also 11 References 12 External links Toggle the table of contents Differential of a function 35 languages العربية Azərbaycanca Беларуская Български Català Чӑвашла Čeština Cymraeg Dansk Deutsch Eesti Español Esperanto Français 客家語/Hak-kâ-ngî 한국어 Bahasa Indonesia Italiano עברית ქართული Қазақша Lietuvių Nederlands 日本語 Norsk bokmål Norsk nynorsk Polski Português Русский Simple English Suomi Svenska தமிழ் Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version Appearance move to sidebar hide From Wikipedia, the free encyclopedia Notion in calculus For other uses of "differential" in mathematics, see Differential (mathematics).

Part of a series of articles about Calculus ∫ a b f ′ ( t ) d t = f ( b ) − f ( a ) {\displaystyle \int _{a}^{b}f'(t)\,dt=f(b)-f(a)} Fundamental theorem Limits Continuity Rolle's theorem Mean value theorem Inverse function theorem Differential Definitions Derivative ( generalizations ) Differential infinitesimal of a function total Concepts Differentiation notation Second derivative Implicit differentiation Logarithmic differentiation Related rates Taylor's theorem Rules and identities Sum Product Chain Power Quotient L'Hôpital's rule Inverse General Leibniz Faà di Bruno's formula Reynolds Integral Lists of integrals Integral transform Leibniz integral rule Definitions Antiderivative Integral ( improper ) Riemann integral Lebesgue integration Contour integration Integral of inverse functions Integration by Parts Discs Cylindrical shells Substitution ( trigonometric , tangent half-angle , Euler ) Euler's formula Partial fractions Changing order Reduction formulae Differentiating under the integral sign Risch algorithm Series Geometric ( arithmetico-geometric ) Harmonic Alternating Power Binomial Taylor Convergence tests Summand limit (term test) Ratio Root Integral Direct comparison Limit comparison Alternating series Cauchy condensation Dirichlet Abel Vector Gradient Divergence Curl Laplacian Directional derivative Identities Theorems Gradient Green's Stokes' Divergence generalized Stokes Helmholtz decomposition Multivariable Formalisms Matrix Tensor Exterior Geometric Definitions Partial derivative Multiple integral Line integral Surface integral Volume integral Jacobian Hessian Advanced Calculus on Euclidean space Generalized functions Limit of distributions Specialized Fractional Malliavin Stochastic Variations Miscellaneous Precalculus History Glossary List of topics Integration Bee Mathematical analysis Nonstandard analysis v t e In calculus , the differential represents the principal part of the change in a function y = f ( x ) {\displaystyle y=f(x)} with respect to changes in the independent variable.

The differential d y {\displaystyle dy} is defined by d y = f ′ ( x ) d x , {\displaystyle dy=f'(x)\,dx,} where f ′ ( x ) {\displaystyle f'(x)} is the derivative of f with respect to x {\displaystyle x} , and d x {\displaystyle dx} is an additional real variable (so that d y {\displaystyle dy} is a function of x {\displaystyle x} and d x {\displaystyle dx} ).

The notation is such that the equation d y = d y d x d x {\displaystyle dy={\frac {dy}{dx}}\,dx} holds, where the derivative is represented in the Leibniz notation d y / d x {\displaystyle dy/dx} , and this is consistent with regarding the derivative as the quotient of the differentials.

One also writes d f ( x ) = f ′ ( x ) d x.

{\displaystyle df(x)=f'(x)\,dx.

} The precise meaning of the variables d y {\displaystyle dy} and d x {\displaystyle dx} depends on the context of the application and the required level of mathematical rigor.

The domain of these variables may take on a particular geometrical significance if the differential is regarded as a particular differential form , or analytical significance if the differential is regarded as a linear approximation to the increment of a function.

Traditionally, the variables d x {\displaystyle dx} and d y {\displaystyle dy} are considered to be very small ( infinitesimal ), and this interpretation is made rigorous in non-standard analysis.

History and usage [ edit ] The differential was first introduced via an intuitive or heuristic definition by Isaac Newton and furthered by Gottfried Leibniz , who thought of the differential dy as an infinitely small (or infinitesimal ) change in the value y of the function, corresponding to an infinitely small change dx in the function's argument x.

For that reason, the instantaneous rate of change of y with respect to x , which is the value of the derivative of the function, is denoted by the fraction d y d x {\displaystyle {\frac {dy}{dx}}} in what is called the Leibniz notation for derivatives.

The quotient d y / d x {\displaystyle dy/dx} is not infinitely small; rather it is a real number.

The use of infinitesimals in this form was widely criticized, for instance by the famous pamphlet The Analyst by Bishop Berkeley.

Augustin-Louis Cauchy ( 1823 ) defined the differential without appeal to the atomism of Leibniz's infinitesimals.

[1] [2] Instead, Cauchy, following d'Alembert , inverted the logical order of Leibniz and his successors: the derivative itself became the fundamental object, defined as a limit of difference quotients, and the differentials were then defined in terms of it.

That is, one was free to define the differential d y {\displaystyle dy} by an expression d y = f ′ ( x ) d x {\displaystyle dy=f'(x)\,dx} in which d y {\displaystyle dy} and d x {\displaystyle dx} are simply new variables taking finite real values, [3] not fixed infinitesimals as they had been for Leibniz.

[4] According to Boyer (1959 , p.

12), Cauchy's approach was a significant logical improvement over the infinitesimal approach of Leibniz because, instead of invoking the metaphysical notion of infinitesimals, the quantities d y {\displaystyle dy} and d x {\displaystyle dx} could now be manipulated in exactly the same manner as any other real quantities in a meaningful way.

Cauchy's overall conceptual approach to differentials remains the standard one in modern analytical treatments, [5] although the final word on rigor, a fully modern notion of the limit, was ultimately due to Karl Weierstrass.

[6] In physical treatments, such as those applied to the theory of thermodynamics , the infinitesimal view still prevails.

Courant & John (1999 , p.

184) reconcile the physical use of infinitesimal differentials with the mathematical impossibility of them as follows.

The differentials represent finite non-zero values that are smaller than the degree of accuracy required for the particular purpose for which they are intended.

Thus "physical infinitesimals" need not appeal to a corresponding mathematical infinitesimal in order to have a precise sense.

Following twentieth-century developments in mathematical analysis and differential geometry , it became clear that the notion of the differential of a function could be extended in a variety of ways.

In real analysis , it is more desirable to deal directly with the differential as the principal part of the increment of a function.

This leads directly to the notion that the differential of a function at a point is a linear functional of an increment Δ x {\displaystyle \Delta x}.

This approach allows the differential (as a linear map) to be developed for a variety of more sophisticated spaces, ultimately giving rise to such notions as the Fréchet or Gateaux derivative.

Likewise, in differential geometry , the differential of a function at a point is a linear function of a tangent vector (an "infinitely small displacement"), which exhibits it as a kind of one-form: the exterior derivative of the function.

In non-standard calculus , differentials are regarded as infinitesimals, which can themselves be put on a rigorous footing (see differential (infinitesimal) ).

Definition [ edit ] The differential of a function f ( x ) {\displaystyle f(x)} at a point x 0 {\displaystyle x_{0}}.

The differential is defined in modern treatments of differential calculus as follows.

[7] The differential of a function f ( x ) {\displaystyle f(x)} of a single real variable x {\displaystyle x} is the function d f {\displaystyle df} of two independent real variables x {\displaystyle x} and Δ x {\displaystyle \Delta x} given by d f ( x , Δ x ) = d e f f ′ ( x ) Δ x.

{\displaystyle df(x,\Delta x)\ {\stackrel {\mathrm {def} }{=}}\ f'(x)\,\Delta x.

} One or both of the arguments may be suppressed, i.

e.

, one may see d f ( x ) {\displaystyle df(x)} or simply d f {\displaystyle df}.

If y = f ( x ) {\displaystyle y=f(x)} , the differential may also be written as d y {\displaystyle dy}.

Since d x ( x , Δ x ) = Δ x {\displaystyle dx(x,\Delta x)=\Delta x} , it is conventional to write d x = Δ x {\displaystyle dx=\Delta x} so that the following equality holds: d f ( x ) = f ′ ( x ) d x {\displaystyle df(x)=f'(x)\,dx} This notion of differential is broadly applicable when a linear approximation to a function is sought, in which the value of the increment Δ x {\displaystyle \Delta x} is small enough.

More precisely, if f {\displaystyle f} is a differentiable function at x {\displaystyle x} , then the difference in y {\displaystyle y} -values Δ y = d e f f ( x + Δ x ) − f ( x ) {\displaystyle \Delta y\ {\stackrel {\rm {def}}{=}}\ f(x+\Delta x)-f(x)} satisfies Δ y = f ′ ( x ) Δ x + ε = d f ( x ) + ε {\displaystyle \Delta y=f'(x)\,\Delta x+\varepsilon =df(x)+\varepsilon \,} where the error ε {\displaystyle \varepsilon } in the approximation satisfies ε / Δ x → 0 {\displaystyle \varepsilon /\Delta x\rightarrow 0} as Δ x → 0 {\displaystyle \Delta x\rightarrow 0}.

In other words, one has the approximate identity Δ y ≈ d y {\displaystyle \Delta y\approx dy} in which the error can be made as small as desired relative to Δ x {\displaystyle \Delta x} by constraining Δ x {\displaystyle \Delta x} to be sufficiently small; that is to say, Δ y − d y Δ x → 0 {\displaystyle {\frac {\Delta y-dy}{\Delta x}}\to 0} as Δ x → 0 {\displaystyle \Delta x\rightarrow 0}.

For this reason, the differential of a function is known as the principal (linear) part in the increment of a function: the differential is a linear function of the increment Δ x {\displaystyle \Delta x} , and although the error ε {\displaystyle \varepsilon } may be nonlinear, it tends to zero rapidly as Δ x {\displaystyle \Delta x} tends to zero.

Differentials in several variables [ edit ] Operator / Function f ( x ) {\displaystyle f(x)} f ( x , y , u ( x , y ) , v ( x , y ) ) {\displaystyle f(x,y,u(x,y),v(x,y))} Differential 1: d f = d e f f x ′ d x {\displaystyle df\,{\overset {\underset {\mathrm {def} }{}}{=}}\,f'_{x}\,dx} 2: d x f = d e f f x ′ d x {\displaystyle d_{x}f\,{\overset {\underset {\mathrm {def} }{}}{=}}\,f'_{x}\,dx} 3: d f = d e f f x ′ d x + f y ′ d y + f u ′ d u + f v ′ d v {\displaystyle df\,{\overset {\underset {\mathrm {def} }{}}{=}}\,f'_{x}dx+f'_{y}dy+f'_{u}du+f'_{v}dv} Partial derivative f x ′ = ( 1 ) d f d x {\displaystyle f'_{x}\,{\overset {\underset {\mathrm {(1)} }{}}{=}}\,{\frac {df}{dx}}} f x ′ = ( 2 ) d x f d x = ∂ f ∂ x {\displaystyle f'_{x}\,{\overset {\underset {\mathrm {(2)} }{}}{=}}\,{\frac {d_{x}f}{dx}}={\frac {\partial f}{\partial x}}} Total derivative d f d x = ( 1 ) f x ′ {\displaystyle {\frac {df}{dx}}\,{\overset {\underset {\mathrm {(1)} }{}}{=}}\,f'_{x}} d f d x = ( 3 ) f x ′ + f u ′ d u d x + f v ′ d v d x ; ( f y ′ d y d x = 0 ) {\displaystyle {\frac {df}{dx}}\,{\overset {\underset {\mathrm {(3)} }{}}{=}}\,f'_{x}+f'_{u}{\frac {du}{dx}}+f'_{v}{\frac {dv}{dx}};(f'_{y}{\frac {dy}{dx}}=0)} Following Goursat (1904 , I, §15), for functions of more than one independent variable, y = f ( x 1 , … , x n ) , {\displaystyle y=f(x_{1},\dots ,x_{n}),} the partial differential of y with respect to any one of the variables x 1 is the principal part of the change in y resulting from a change dx 1 in that one variable.

The partial differential is therefore ∂ y ∂ x 1 d x 1 {\displaystyle {\frac {\partial y}{\partial x_{1}}}dx_{1}} involving the partial derivative of y with respect to x 1.

The sum of the partial differentials with respect to all of the independent variables is the total differential d y = ∂ y ∂ x 1 d x 1 + ⋯ + ∂ y ∂ x n d x n , {\displaystyle dy={\frac {\partial y}{\partial x_{1}}}dx_{1}+\cdots +{\frac {\partial y}{\partial x_{n}}}dx_{n},} which is the principal part of the change in y resulting from changes in the independent variables x i.

More precisely, in the context of multivariable calculus, following Courant (1937b) , if f is a differentiable function, then by the definition of differentiability , the increment Δ y = d e f f ( x 1 + Δ x 1 , … , x n + Δ x n ) − f ( x 1 , … , x n ) = ∂ y ∂ x 1 Δ x 1 + ⋯ + ∂ y ∂ x n Δ x n + ε 1 Δ x 1 + ⋯ + ε n Δ x n {\displaystyle {\begin{aligned}\Delta y&{}~{\stackrel {\mathrm {def} }{=}}~f(x_{1}+\Delta x_{1},\dots ,x_{n}+\Delta x_{n})-f(x_{1},\dots ,x_{n})\\&{}={\frac {\partial y}{\partial x_{1}}}\Delta x_{1}+\cdots +{\frac {\partial y}{\partial x_{n}}}\Delta x_{n}+\varepsilon _{1}\Delta x_{1}+\cdots +\varepsilon _{n}\Delta x_{n}\end{aligned}}} where the error terms ε i tend to zero as the increments Δ x i jointly tend to zero.

The total differential is then rigorously defined as d y = ∂ y ∂ x 1 Δ x 1 + ⋯ + ∂ y ∂ x n Δ x n.

{\displaystyle dy={\frac {\partial y}{\partial x_{1}}}\Delta x_{1}+\cdots +{\frac {\partial y}{\partial x_{n}}}\Delta x_{n}.

} Since, with this definition, d x i ( Δ x 1 , … , Δ x n ) = Δ x i , {\displaystyle dx_{i}(\Delta x_{1},\dots ,\Delta x_{n})=\Delta x_{i},} one has d y = ∂ y ∂ x 1 d x 1 + ⋯ + ∂ y ∂ x n d x n.

{\displaystyle dy={\frac {\partial y}{\partial x_{1}}}\,dx_{1}+\cdots +{\frac {\partial y}{\partial x_{n}}}\,dx_{n}.

} As in the case of one variable, the approximate identity holds d y ≈ Δ y {\displaystyle dy\approx \Delta y} in which the total error can be made as small as desired relative to Δ x 1 2 + ⋯ + Δ x n 2 {\textstyle {\sqrt {\Delta x_{1}^{2}+\cdots +\Delta x_{n}^{2}}}} by confining attention to sufficiently small increments.

Application of the total differential to error estimation [ edit ] In measurement, the total differential is used in estimating the error Δ f {\displaystyle \Delta f} of a function f {\displaystyle f} based on the errors Δ x , Δ y , … {\displaystyle \Delta x,\Delta y,\ldots } of the parameters x , y , … {\displaystyle x,y,\ldots }.

Assuming that the interval is short enough for the change to be approximately linear: Δ f ( x ) = f ′ ( x ) Δ x {\displaystyle \Delta f(x)=f'(x)\Delta x} and that all variables are independent, then for all variables, Δ f = f x Δ x + f y Δ y + ⋯ {\displaystyle \Delta f=f_{x}\Delta x+f_{y}\Delta y+\cdots } This is because the derivative f x {\displaystyle f_{x}} with respect to the particular parameter x {\displaystyle x} gives the sensitivity of the function f {\displaystyle f} to a change in x {\displaystyle x} , in particular the error Δ x {\displaystyle \Delta x}.

As they are assumed to be independent, the analysis describes the worst-case scenario.

The absolute values of the component errors are used, because after simple computation, the derivative may have a negative sign.

From this principle the error rules of summation, multiplication etc.

are derived, e.

g.

: Let f ( a , b ) = a b {\displaystyle f(a,b)=ab}.

Then, the finite error can be approximated as Δ f = f a Δ a + f b Δ b.

{\displaystyle \Delta f=f_{a}\Delta a+f_{b}\Delta b.

} Evaluating the derivatives: Δ f = b Δ a + a Δ b.

{\displaystyle \Delta f=b\Delta a+a\Delta b.

} Dividing by f , which is a × b Δ f f = Δ a a + Δ b b {\displaystyle {\frac {\Delta f}{f}}={\frac {\Delta a}{a}}+{\frac {\Delta b}{b}}} That is to say, in multiplication, the total relative error is the sum of the relative errors of the parameters.

To illustrate how this depends on the function considered, consider the case where the function is f ( a , b ) = a ln ⁡ b {\displaystyle f(a,b)=a\ln b} instead.

Then, it can be computed that the error estimate is Δ f f = Δ a a + Δ b b ln ⁡ b {\displaystyle {\frac {\Delta f}{f}}={\frac {\Delta a}{a}}+{\frac {\Delta b}{b\ln b}}} with an extra ' ln b ' factor not found in the case of a simple product.

This additional factor tends to make the error smaller, as ln b is not as large as a bare b.

Higher-order differentials [ edit ] Higher-order differentials of a function y = f ( x ) of a single variable x can be defined via: [8] d 2 y = d ( d y ) = d ( f ′ ( x ) d x ) = ( d f ′ ( x ) ) d x = f ″ ( x ) ( d x ) 2 , {\displaystyle d^{2}y=d(dy)=d(f'(x)dx)=(df'(x))dx=f''(x)\,(dx)^{2},} and, in general, d n y = f ( n ) ( x ) ( d x ) n.

{\displaystyle d^{n}y=f^{(n)}(x)\,(dx)^{n}.

} Informally, this motivates Leibniz's notation for higher-order derivatives f ( n ) ( x ) = d n f d x n.

{\displaystyle f^{(n)}(x)={\frac {d^{n}f}{dx^{n}}}.

} When the independent variable x itself is permitted to depend on other variables, then the expression becomes more complicated, as it must include also higher order differentials in x itself.

Thus, for instance, d 2 y = f ″ ( x ) ( d x ) 2 + f ′ ( x ) d 2 x d 3 y = f ‴ ( x ) ( d x ) 3 + 3 f ″ ( x ) d x d 2 x + f ′ ( x ) d 3 x {\displaystyle {\begin{aligned}d^{2}y&=f''(x)\,(dx)^{2}+f'(x)d^{2}x\\[1ex]d^{3}y&=f'''(x)\,(dx)^{3}+3f''(x)dx\,d^{2}x+f'(x)d^{3}x\end{aligned}}} and so forth.

Similar considerations apply to defining higher order differentials of functions of several variables.

For example, if f is a function of two variables x and y , then d n f = ∑ k = 0 n ( n k ) ∂ n f ∂ x k ∂ y n − k ( d x ) k ( d y ) n − k , {\displaystyle d^{n}f=\sum _{k=0}^{n}{\binom {n}{k}}{\frac {\partial ^{n}f}{\partial x^{k}\partial y^{n-k}}}(dx)^{k}(dy)^{n-k},} where ( n k ) {\textstyle {\binom {n}{k}}} is a binomial coefficient.

In more variables, an analogous expression holds, but with an appropriate multinomial expansion rather than binomial expansion.

[9] Higher order differentials in several variables also become more complicated when the independent variables are themselves allowed to depend on other variables.

For instance, for a function f of x and y which are allowed to depend on auxiliary variables, one has d 2 f = ( ∂ 2 f ∂ x 2 ( d x ) 2 + 2 ∂ 2 f ∂ x ∂ y d x d y + ∂ 2 f ∂ y 2 ( d y ) 2 ) + ∂ f ∂ x d 2 x + ∂ f ∂ y d 2 y.

{\displaystyle d^{2}f=\left({\frac {\partial ^{2}f}{\partial x^{2}}}(dx)^{2}+2{\frac {\partial ^{2}f}{\partial x\partial y}}dx\,dy+{\frac {\partial ^{2}f}{\partial y^{2}}}(dy)^{2}\right)+{\frac {\partial f}{\partial x}}d^{2}x+{\frac {\partial f}{\partial y}}d^{2}y.

} Because of this notational awkwardness, the use of higher order differentials was roundly criticized by Hadamard (1935) , who concluded: Enfin, que signifie ou que représente l'égalité d 2 z = r d x 2 + 2 s d x d y + t d y 2 ? {\displaystyle d^{2}z=r\,dx^{2}+2s\,dx\,dy+t\,dy^{2}\,?} A mon avis, rien du tout.

That is: Finally, what is meant, or represented, by the equality [.

]? In my opinion, nothing at all.

In spite of this skepticism, higher order differentials did emerge as an important tool in analysis.

[10] In these contexts, the n -th order differential of the function f applied to an increment Δ x is defined by d n f ( x , Δ x ) = d n d t n f ( x + t Δ x ) | t = 0 {\displaystyle d^{n}f(x,\Delta x)=\left.

{\frac {d^{n}}{dt^{n}}}f(x+t\Delta x)\right|_{t=0}} or an equivalent expression, such as lim t → 0 Δ t Δ x n f t n {\displaystyle \lim _{t\to 0}{\frac {\Delta _{t\Delta x}^{n}f}{t^{n}}}} where Δ t Δ x n f {\displaystyle \Delta _{t\Delta x}^{n}f} is an n th forward difference with increment t Δ x.

This definition makes sense as well if f is a function of several variables (for simplicity taken here as a vector argument).

Then the n -th differential defined in this way is a homogeneous function of degree n in the vector increment Δ x.

Furthermore, the Taylor series of f at the point x is given by f ( x + Δ x ) ∼ f ( x ) + d f ( x , Δ x ) + 1 2 d 2 f ( x , Δ x ) + ⋯ + 1 n ! d n f ( x , Δ x ) + ⋯ {\displaystyle f(x+\Delta x)\sim f(x)+df(x,\Delta x)+{\frac {1}{2}}d^{2}f(x,\Delta x)+\cdots +{\frac {1}{n!}}d^{n}f(x,\Delta x)+\cdots } The higher order Gateaux derivative generalizes these considerations to infinite dimensional spaces.

Properties [ edit ] A number of properties of the differential follow in a straightforward manner from the corresponding properties of the derivative, partial derivative, and total derivative.

These include: [11] Linearity : For constants a and b and differentiable functions f and g , d ( a f + b g ) = a d f + b d g.

{\displaystyle d(af+bg)=a\,df+b\,dg.

} Product rule : For two differentiable functions f and g , d ( f g ) = f d g + g d f.

{\displaystyle d(fg)=f\,dg+g\,df.

} An operation d with these two properties is known in abstract algebra as a derivation.

They imply the power rule d ( f n ) = n f n − 1 d f {\displaystyle d(f^{n})=nf^{n-1}df} In addition, various forms of the chain rule hold, in increasing level of generality: [12] If y = f ( u ) is a differentiable function of the variable u and u = g ( x ) is a differentiable function of x , then d y = f ′ ( u ) d u = f ′ ( g ( x ) ) g ′ ( x ) d x.

{\displaystyle dy=f'(u)\,du=f'(g(x))g'(x)\,dx.

} If y = f ( x 1 ,.

, x n ) and all of the variables x 1 ,.

, x n depend on another variable t , then by the chain rule for partial derivatives , one has d y = d y d t d t = ∂ y ∂ x 1 d x 1 + ⋯ + ∂ y ∂ x n d x n = ∂ y ∂ x 1 d x 1 d t d t + ⋯ + ∂ y ∂ x n d x n d t d t.

{\displaystyle {\begin{aligned}dy={\frac {dy}{dt}}dt&={\frac {\partial y}{\partial x_{1}}}dx_{1}+\cdots +{\frac {\partial y}{\partial x_{n}}}dx_{n}\\[1ex]&={\frac {\partial y}{\partial x_{1}}}{\frac {dx_{1}}{dt}}\,dt+\cdots +{\frac {\partial y}{\partial x_{n}}}{\frac {dx_{n}}{dt}}\,dt.

\end{aligned}}} Heuristically, the chain rule for several variables can itself be understood by dividing through both sides of this equation by the infinitely small quantity dt.

More general analogous expressions hold, in which the intermediate variables x i depend on more than one variable.

General formulation [ edit ] See also: Fréchet derivative and Gateaux derivative A consistent notion of differential can be developed for a function f : R n → R m between two Euclidean spaces.

Let x ,Δ x ∈ R n be a pair of Euclidean vectors.

The increment in the function f is Δ f = f ( x + Δ x ) − f ( x ).

{\displaystyle \Delta f=f(\mathbf {x} +\Delta \mathbf {x} )-f(\mathbf {x} ).

} If there exists an m × n matrix A such that Δ f = A Δ x + ‖ Δ x ‖ ε {\displaystyle \Delta f=A\Delta \mathbf {x} +\|\Delta \mathbf {x} \|{\boldsymbol {\varepsilon }}} in which the vector ε → 0 as Δ x → 0 , then f is by definition differentiable at the point x.

The matrix A is sometimes known as the Jacobian matrix , and the linear transformation that associates to the increment Δ x ∈ R n the vector A Δ x ∈ R m is, in this general setting, known as the differential df ( x ) of f at the point x.

This is precisely the Fréchet derivative , and the same construction can be made to work for a function between any Banach spaces.

Another fruitful point of view is to define the differential directly as a kind of directional derivative : d f ( x , h ) = lim t → 0 f ( x + t h ) − f ( x ) t = d d t f ( x + t h ) | t = 0 , {\displaystyle df(\mathbf {x} ,\mathbf {h} )=\lim _{t\to 0}{\frac {f(\mathbf {x} +t\mathbf {h} )-f(\mathbf {x} )}{t}}=\left.

{\frac {d}{dt}}f(\mathbf {x} +t\mathbf {h} )\right|_{t=0},} which is the approach already taken for defining higher order differentials (and is most nearly the definition set forth by Cauchy).

If t represents time and x position, then h represents a velocity instead of a displacement as we have heretofore regarded it.

This yields yet another refinement of the notion of differential: that it should be a linear function of a kinematic velocity.

The set of all velocities through a given point of space is known as the tangent space , and so df gives a linear function on the tangent space: a differential form.

With this interpretation, the differential of f is known as the exterior derivative , and has broad application in differential geometry because the notion of velocities and the tangent space makes sense on any differentiable manifold.

If, in addition, the output value of f also represents a position (in a Euclidean space), then a dimensional analysis confirms that the output value of df must be a velocity.

If one treats the differential in this manner, then it is known as the pushforward since it "pushes" velocities from a source space into velocities in a target space.

Other approaches [ edit ] Main article: Differential (infinitesimal) Although the notion of having an infinitesimal increment dx is not well-defined in modern mathematical analysis , a variety of techniques exist for defining the infinitesimal differential so that the differential of a function can be handled in a manner that does not clash with the Leibniz notation.

These include: Defining the differential as a kind of differential form , specifically the exterior derivative of a function.

The infinitesimal increments are then identified with vectors in the tangent space at a point.

This approach is popular in differential geometry and related fields, because it readily generalizes to mappings between differentiable manifolds.

Differentials as nilpotent elements of commutative rings.

This approach is popular in algebraic geometry.

[13] Differentials in smooth models of set theory.

This approach is known as synthetic differential geometry or smooth infinitesimal analysis and is closely related to the algebraic geometric approach, except that ideas from topos theory are used to hide the mechanisms by which nilpotent infinitesimals are introduced.

[14] Differentials as infinitesimals in hyperreal number systems, which are extensions of the real numbers which contain invertible infinitesimals and infinitely large numbers.

This is the approach of nonstandard analysis pioneered by Abraham Robinson.

[15] Examples and applications [ edit ] Differentials may be effectively used in numerical analysis to study the propagation of experimental errors in a calculation, and thus the overall numerical stability of a problem ( Courant 1937a ).

Suppose that the variable x represents the outcome of an experiment and y is the result of a numerical computation applied to x.

The question is to what extent errors in the measurement of x influence the outcome of the computation of y.

If the x is known to within Δ x of its true value, then Taylor's theorem gives the following estimate on the error Δ y in the computation of y : Δ y = f ′ ( x ) Δ x + ( Δ x ) 2 2 f ″ ( ξ ) {\displaystyle \Delta y=f'(x)\Delta x+{\frac {(\Delta x)^{2}}{2}}f''(\xi )} where ξ = x + θ Δ x for some 0 < θ < 1.

If Δ x is small, then the second order term is negligible, so that Δ y is, for practical purposes, well-approximated by dy = f' ( x ) Δ x.

The differential is often useful to rewrite a differential equation d y d x = g ( x ) {\displaystyle {\frac {dy}{dx}}=g(x)} in the form d y = g ( x ) d x , {\displaystyle dy=g(x)\,dx,} in particular when one wants to separate the variables.

Notes [ edit ] ^ For a detailed historical account of the differential, see Boyer 1959 , especially page 275 for Cauchy's contribution on the subject.

An abbreviated account appears in Kline 1972 , Chapter 40.

^ Cauchy explicitly denied the possibility of actual infinitesimal and infinite quantities ( Boyer 1959 , pp.

273–275), and took the radically different point of view that "a variable quantity becomes infinitely small when its numerical value decreases indefinitely in such a way as to converge to zero" ( Cauchy 1823 , p.

12; translation from Boyer 1959 , p.

273).

^ Boyer 1959 , p.

275 ^ Boyer 1959 , p.

12: "The differentials as thus defined are only new variables , and not fixed infinitesimals.

" ^ Courant 1937a , II, §9: "Here we remark merely in passing that it is possible to use this approximate representation of the increment Δ y {\displaystyle \Delta y} by the linear expression h f ( x ) {\displaystyle hf(x)} to construct a logically satisfactory definition of a "differential", as was done by Cauchy in particular.

" ^ Boyer 1959 , p.

284 ^ See, for instance, the influential treatises of Courant 1937a , Kline 1977 , Goursat 1904 , and Hardy 1908.

Tertiary sources for this definition include also Tolstov 2001 and Itô 1993 , §106.

^ Cauchy 1823.

See also, for instance, Goursat 1904 , I, §14.

^ Goursat 1904 , I, §14 ^ In particular to infinite dimensional holomorphy ( Hille & Phillips 1974 ) and numerical analysis via the calculus of finite differences.

^ Goursat 1904 , I, §17 ^ Goursat 1904 , I, §§14,16 ^ Eisenbud & Harris 1998.

^ See Kock 2006 and Moerdijk & Reyes 1991.

^ See Robinson 1996 and Keisler 1986.

See also [ edit ] Notation for differentiation References [ edit ] Boyer, Carl B.

(1959), The history of the calculus and its conceptual development , New York: Dover Publications , MR 0124178.

Cauchy, Augustin-Louis (1823), Résumé des Leçons données à l'Ecole royale polytechnique sur les applications du calcul infinitésimal , archived from the original on 2007-07-08 , retrieved 2009-08-19.

Courant, Richard (1937a), Differential and integral calculus.

Vol.

I , Wiley Classics Library, New York: John Wiley & Sons (published 1988), ISBN 978-0-471-60842-4 , MR 1009558.

Courant, Richard (1937b), Differential and integral calculus.

Vol.

II , Wiley Classics Library, New York: John Wiley & Sons (published 1988), ISBN 978-0-471-60840-0 , MR 1009559.

Courant, Richard ; John, Fritz (1999), Introduction to Calculus and Analysis Volume 1 , Classics in Mathematics, Berlin, New York: Springer-Verlag , ISBN 3-540-65058-X , MR 1746554 Eisenbud, David ; Harris, Joe (1998), The Geometry of Schemes , Springer-Verlag, ISBN 0-387-98637-5.

Fréchet, Maurice (1925), "La notion de différentielle dans l'analyse générale", Annales Scientifiques de l'École Normale Supérieure , Série 3, 42 : 293–323, doi : 10.

24033/asens.

766 , ISSN 0012-9593 , MR 1509268.

Goursat, Édouard (1904), A course in mathematical analysis: Vol 1: Derivatives and differentials, definite integrals, expansion in series, applications to geometry , E.

R.

Hedrick, New York: Dover Publications (published 1959), MR 0106155.

Hadamard, Jacques (1935), "La notion de différentiel dans l'enseignement", Mathematical Gazette , XIX (236): 341–342, doi : 10.

2307/3606323 , JSTOR 3606323.

Hardy, Godfrey Harold (1908), A Course of Pure Mathematics , Cambridge University Press , ISBN 978-0-521-09227-2.

Hille, Einar ; Phillips, Ralph S.

(1974), Functional analysis and semi-groups , Providence, R.

I.

: American Mathematical Society , MR 0423094.

Itô, Kiyosi (1993), Encyclopedic Dictionary of Mathematics (2nd ed.

), MIT Press , ISBN 978-0-262-59020-4.

Kline, Morris (1977), "Chapter 13: Differentials and the law of the mean", Calculus: An intuitive and physical approach , John Wiley and Sons.

Kline, Morris (1972), Mathematical thought from ancient to modern times (3rd ed.

), Oxford University Press (published 1990), ISBN 978-0-19-506136-9 Keisler, H.

Jerome (1986), Elementary Calculus: An Infinitesimal Approach (2nd ed.

).

Kock, Anders (2006), Synthetic Differential Geometry (PDF) (2nd ed.

), Cambridge University Press.

Moerdijk, I.

; Reyes, G.

E.

(1991), Models for Smooth Infinitesimal Analysis , Springer-Verlag.

Robinson, Abraham (1996), Non-standard analysis , Princeton University Press , ISBN 978-0-691-04490-3.

Tolstov, G.

P.

(2001) [1994], "Differential" , Encyclopedia of Mathematics , EMS Press.

External links [ edit ] Differential Of A Function at Wolfram Demonstrations Project Retrieved from " https://en.

wikipedia.

org/w/index.

php?title=Differential_of_a_function&oldid=1225888577 " Categories : Differential calculus Generalizations of the derivative Linear operators in calculus Hidden categories: Articles with short description Short description matches Wikidata Pages using sidebar with the child parameter This page was last edited on 27 May 2024, at 09:43 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 4.

0 ; additional terms may apply.

By using this site, you agree to the Terms of Use and Privacy Policy.

Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view.