Gauss–Newton algorithm - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Donate Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Create account Log in Personal tools Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Description 2 Notes 3 Example 4 Convergence properties 5 Solving overdetermined systems of equations 6 Derivation from Newton's method 7 Improved versions 8 Large-scale optimization 9 Related algorithms 10 Example implementations Toggle Example implementations subsection 10.

1 Julia 11 Notes 12 References 13 External links Toggle External links subsection 13.

1 Implementations Toggle the table of contents Gauss–Newton algorithm 14 languages العربية Български Català Deutsch Español Français Bahasa Indonesia Italiano עברית 日本語 Norsk nynorsk Português Русский Svenska Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version Appearance move to sidebar hide From Wikipedia, the free encyclopedia Mathematical algorithm Fitting of a noisy curve by an asymmetrical peak model f β ( x ) {\displaystyle f_{\beta }(x)} with parameters β {\displaystyle \beta } by mimimizing the sum of squared residuals r i ( β ) = y i − f β ( x i ) {\displaystyle r_{i}(\beta )=y_{i}-f_{\beta }(x_{i})} at grid points x i {\displaystyle x_{i}} , using the Gauss–Newton algorithm.

Top: Raw data and model.

Bottom: Evolution of the normalised sum of the squares of the errors.

The Gauss–Newton algorithm is used to solve non-linear least squares problems, which is equivalent to minimizing a sum of squared function values.

It is an extension of Newton's method for finding a minimum of a non-linear function.

Since a sum of squares must be nonnegative, the algorithm can be viewed as using Newton's method to iteratively approximate zeroes of the components of the sum, and thus minimizing the sum.

In this sense, the algorithm is also an effective method for solving overdetermined systems of equations.

It has the advantage that second derivatives, which can be challenging to compute, are not required.

[1] Non-linear least squares problems arise, for instance, in non-linear regression , where parameters in a model are sought such that the model is in good agreement with available observations.

The method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton , and first appeared in Gauss's 1809 work Theoria motus corporum coelestium in sectionibus conicis solem ambientum.

[2] Description [ edit ] Given m {\displaystyle m} functions r = ( r 1 , … , r m ) {\displaystyle {\textbf {r}}=(r_{1},\ldots ,r_{m})} (often called residuals) of n {\displaystyle n} variables β = ( β 1 , … β n ) , {\displaystyle {\boldsymbol {\beta }}=(\beta _{1},\ldots \beta _{n}),} with m ≥ n , {\displaystyle m\geq n,} the Gauss–Newton algorithm iteratively finds the value of β {\displaystyle \beta } that minimize the sum of squares [3] S ( β ) = ∑ i = 1 m r i ( β ) 2.

{\displaystyle S({\boldsymbol {\beta }})=\sum _{i=1}^{m}r_{i}({\boldsymbol {\beta }})^{2}.

} Starting with an initial guess β ( 0 ) {\displaystyle {\boldsymbol {\beta }}^{(0)}} for the minimum, the method proceeds by the iterations β ( s + 1 ) = β ( s ) − ( J r T J r ) − 1 J r T r ( β ( s ) ) , {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\left(\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {J_{r}} \right)^{-1}\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right),} where, if r and β are column vectors , the entries of the Jacobian matrix are ( J r ) i j = ∂ r i ( β ( s ) ) ∂ β j , {\displaystyle \left(\mathbf {J_{r}} \right)_{ij}={\frac {\partial r_{i}\left({\boldsymbol {\beta }}^{(s)}\right)}{\partial \beta _{j}}},} and the symbol T {\displaystyle ^{\operatorname {T} }} denotes the matrix transpose.

At each iteration, the update Δ = β ( s + 1 ) − β ( s ) {\displaystyle \Delta ={\boldsymbol {\beta }}^{(s+1)}-{\boldsymbol {\beta }}^{(s)}} can be found by rearranging the previous equation in the following two steps: Δ = − ( J r T J r ) − 1 J r T r ( β ( s ) ) {\displaystyle \Delta =-\left(\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {J_{r}} \right)^{-1}\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right)} J r T J r Δ = − J r T r ( β ( s ) ) {\displaystyle \mathbf {J_{r}} ^{\operatorname {T} }\mathbf {J_{r}} \Delta =-\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right)} With substitutions A = J r T J r {\textstyle A=\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {J_{r}} } , b = − J r T r ( β ( s ) ) {\displaystyle \mathbf {b} =-\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right)} , and x = Δ {\displaystyle \mathbf {x} =\Delta } , this turns into the conventional matrix equation of form A x = b {\displaystyle A\mathbf {x} =\mathbf {b} } , which can then be solved in a variety of methods (see Notes ).

If m = n , the iteration simplifies to β ( s + 1 ) = β ( s ) − ( J r ) − 1 r ( β ( s ) ) , {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\left(\mathbf {J_{r}} \right)^{-1}\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right),} which is a direct generalization of Newton's method in one dimension.

In data fitting, where the goal is to find the parameters β {\displaystyle {\boldsymbol {\beta }}} such that a given model function f ( x , β ) {\displaystyle \mathbf {f} (\mathbf {x} ,{\boldsymbol {\beta }})} best fits some data points ( x i , y i ) {\displaystyle (x_{i},y_{i})} , the functions r i {\displaystyle r_{i}} are the residuals : r i ( β ) = y i − f ( x i , β ).

{\displaystyle r_{i}({\boldsymbol {\beta }})=y_{i}-f\left(x_{i},{\boldsymbol {\beta }}\right).

} Then, the Gauss–Newton method can be expressed in terms of the Jacobian J f = − J r {\displaystyle \mathbf {J_{f}} =-\mathbf {J_{r}} } of the function f {\displaystyle \mathbf {f} } as β ( s + 1 ) = β ( s ) + ( J f T J f ) − 1 J f T r ( β ( s ) ).

{\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}+\left(\mathbf {J_{f}} ^{\operatorname {T} }\mathbf {J_{f}} \right)^{-1}\mathbf {J_{f}} ^{\operatorname {T} }\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right).

} Note that ( J f T J f ) − 1 J f T {\displaystyle \left(\mathbf {J_{f}} ^{\operatorname {T} }\mathbf {J_{f}} \right)^{-1}\mathbf {J_{f}} ^{\operatorname {T} }} is the left pseudoinverse of J f {\displaystyle \mathbf {J_{f}} }.

Notes [ edit ] The assumption m ≥ n in the algorithm statement is necessary, as otherwise the matrix J r T J r {\displaystyle \mathbf {J_{r}} ^{T}\mathbf {J_{r}} } is not invertible and the normal equations cannot be solved (at least uniquely).

The Gauss–Newton algorithm can be derived by linearly approximating the vector of functions r i.

Using Taylor's theorem , we can write at every iteration: r ( β ) ≈ r ( β ( s ) ) + J r ( β ( s ) ) Δ {\displaystyle \mathbf {r} ({\boldsymbol {\beta }})\approx \mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right)+\mathbf {J_{r}} \left({\boldsymbol {\beta }}^{(s)}\right)\Delta } with Δ = β − β ( s ) {\displaystyle \Delta ={\boldsymbol {\beta }}-{\boldsymbol {\beta }}^{(s)}}.

The task of finding Δ {\displaystyle \Delta } minimizing the sum of squares of the right-hand side; i.

e.

, min ‖ r ( β ( s ) ) + J r ( β ( s ) ) Δ ‖ 2 2 , {\displaystyle \min \left\|\mathbf {r} \left({\boldsymbol {\beta }}^{(s)}\right)+\mathbf {J_{r}} \left({\boldsymbol {\beta }}^{(s)}\right)\Delta \right\|_{2}^{2},} is a linear least-squares problem, which can be solved explicitly, yielding the normal equations in the algorithm.

The normal equations are n simultaneous linear equations in the unknown increments Δ {\displaystyle \Delta }.

They may be solved in one step, using Cholesky decomposition , or, better, the QR factorization of J r {\displaystyle \mathbf {J_{r}} }.

For large systems, an iterative method , such as the conjugate gradient method, may be more efficient.

If there is a linear dependence between columns of J r , the iterations will fail, as J r T J r {\displaystyle \mathbf {J_{r}} ^{T}\mathbf {J_{r}} } becomes singular.

When r {\displaystyle \mathbf {r} } is complex r : C n → C {\displaystyle \mathbf {r} :\mathbb {C} ^{n}\to \mathbb {C} } the conjugate form should be used: ( J r ¯ T J r ) − 1 J r ¯ T {\displaystyle \left({\overline {\mathbf {J_{r}} }}^{\operatorname {T} }\mathbf {J_{r}} \right)^{-1}{\overline {\mathbf {J_{r}} }}^{\operatorname {T} }}.

Example [ edit ] Calculated curve obtained with β ^ 1 = 0.

362 {\displaystyle {\hat {\beta }}_{1}=0.

362} and β ^ 2 = 0.

556 {\displaystyle {\hat {\beta }}_{2}=0.

556} (in blue) versus the observed data (in red) In this example, the Gauss–Newton algorithm will be used to fit a model to some data by minimizing the sum of squares of errors between the data and model's predictions.

In a biology experiment studying the relation between substrate concentration [ S ] and reaction rate in an enzyme-mediated reaction, the data in the following table were obtained.

i 1 2 3 4 5 6 7 [ S ] 0.

038 0.

194 0.

425 0.

626 1.

253 2.

500 3.

740 Rate 0.

050 0.

127 0.

094 0.

2122 0.

2729 0.

2665 0.

3317 It is desired to find a curve (model function) of the form rate = V max ⋅ [ S ] K M + [ S ] {\displaystyle {\text{rate}}={\frac {V_{\text{max}}\cdot [S]}{K_{M}+[S]}}} that fits best the data in the least-squares sense, with the parameters V max {\displaystyle V_{\text{max}}} and K M {\displaystyle K_{M}} to be determined.

Denote by x i {\displaystyle x_{i}} and y i {\displaystyle y_{i}} the values of [ S ] and rate respectively, with i = 1 , … , 7 {\displaystyle i=1,\dots ,7}.

Let β 1 = V max {\displaystyle \beta _{1}=V_{\text{max}}} and β 2 = K M {\displaystyle \beta _{2}=K_{M}}.

We will find β 1 {\displaystyle \beta _{1}} and β 2 {\displaystyle \beta _{2}} such that the sum of squares of the residuals r i = y i − β 1 x i β 2 + x i , ( i = 1 , … , 7 ) {\displaystyle r_{i}=y_{i}-{\frac {\beta _{1}x_{i}}{\beta _{2}+x_{i}}},\quad (i=1,\dots ,7)} is minimized.

The Jacobian J r {\displaystyle \mathbf {J_{r}} } of the vector of residuals r i {\displaystyle r_{i}} with respect to the unknowns β j {\displaystyle \beta _{j}} is a 7 × 2 {\displaystyle 7\times 2} matrix with the i {\displaystyle i} -th row having the entries ∂ r i ∂ β 1 = − x i β 2 + x i ; ∂ r i ∂ β 2 = β 1 ⋅ x i ( β 2 + x i ) 2.

{\displaystyle {\frac {\partial r_{i}}{\partial \beta _{1}}}=-{\frac {x_{i}}{\beta _{2}+x_{i}}};\quad {\frac {\partial r_{i}}{\partial \beta _{2}}}={\frac {\beta _{1}\cdot x_{i}}{\left(\beta _{2}+x_{i}\right)^{2}}}.

} Starting with the initial estimates of β 1 = 0.

9 {\displaystyle \beta _{1}=0.

9} and β 2 = 0.

2 {\displaystyle \beta _{2}=0.

2} , after five iterations of the Gauss–Newton algorithm, the optimal values β ^ 1 = 0.

362 {\displaystyle {\hat {\beta }}_{1}=0.

362} and β ^ 2 = 0.

556 {\displaystyle {\hat {\beta }}_{2}=0.

556} are obtained.

The sum of squares of residuals decreased from the initial value of 1.

445 to 0.

00784 after the fifth iteration.

The plot in the figure on the right shows the curve determined by the model for the optimal parameters with the observed data.

Convergence properties [ edit ] The Gauss-Newton iteration is guaranteed to converge toward a local minimum point β ^ {\displaystyle {\hat {\beta }}} under 4 conditions: [4] The functions r 1 , … , r m {\displaystyle r_{1},\ldots ,r_{m}} are twice continuously differentiable in an open convex set D ∋ β ^ {\displaystyle D\ni {\hat {\beta }}} , the Jacobian J r ( β ^ ) {\displaystyle \mathbf {J} _{\mathbf {r} }({\hat {\beta }})} is of full column rank, the initial iterate β ( 0 ) {\displaystyle \beta ^{(0)}} is near β ^ {\displaystyle {\hat {\beta }}} , and the local minimum value | S ( β ^ ) | {\displaystyle |S({\hat {\beta }})|} is small.

The convergence is quadratic if | S ( β ^ ) | = 0 {\displaystyle |S({\hat {\beta }})|=0}.

It can be shown [5] that the increment Δ is a descent direction for S , and, if the algorithm converges, then the limit is a stationary point of S.

For large minimum value | S ( β ^ ) | {\displaystyle |S({\hat {\beta }})|} , however, convergence is not guaranteed, not even local convergence as in Newton's method , or convergence under the usual Wolfe conditions.

[6] The rate of convergence of the Gauss–Newton algorithm can approach quadratic.

[7] The algorithm may converge slowly or not at all if the initial guess is far from the minimum or the matrix J r T J r {\displaystyle \mathbf {J_{r}^{\operatorname {T} }J_{r}} } is ill-conditioned.

For example, consider the problem with m = 2 {\displaystyle m=2} equations and n = 1 {\displaystyle n=1} variable, given by r 1 ( β ) = β + 1 , r 2 ( β ) = λ β 2 + β − 1.

{\displaystyle {\begin{aligned}r_{1}(\beta )&=\beta +1,\\r_{2}(\beta )&=\lambda \beta ^{2}+\beta -1.

\end{aligned}}} The optimum is at β = 0 {\displaystyle \beta =0}.

(Actually the optimum is at β = − 1 {\displaystyle \beta =-1} for λ = 2 {\displaystyle \lambda =2} , because S ( 0 ) = 1 2 + ( − 1 ) 2 = 2 {\displaystyle S(0)=1^{2}+(-1)^{2}=2} , but S ( − 1 ) = 0 {\displaystyle S(-1)=0}.

) If λ = 0 {\displaystyle \lambda =0} , then the problem is in fact linear and the method finds the optimum in one iteration.

If |λ| < 1, then the method converges linearly and the error decreases asymptotically with a factor |λ| at every iteration.

However, if |λ| > 1, then the method does not even converge locally.

[8] Solving overdetermined systems of equations [ edit ] The Gauss-Newton iteration x ( k + 1 ) = x ( k ) − J ( x ( k ) ) † f ( x ( k ) ) , k = 0 , 1 , … {\displaystyle \mathbf {x} ^{(k+1)}=\mathbf {x} ^{(k)}-J(\mathbf {x} ^{(k)})^{\dagger }\mathbf {f} (\mathbf {x} ^{(k)})\,,\quad k=0,1,\ldots } is an effective method for solving overdetermined systems of equations in the form of f ( x ) = 0 {\displaystyle \mathbf {f} (\mathbf {x} )=\mathbf {0} } with f ( x ) = [ f 1 ( x 1 , … , x n ) ⋮ f m ( x 1 , … , x n ) ] {\displaystyle \mathbf {f} (\mathbf {x} )={\begin{bmatrix}f_{1}(x_{1},\ldots ,x_{n})\\\vdots \\f_{m}(x_{1},\ldots ,x_{n})\end{bmatrix}}} and m > n {\displaystyle m>n} where J ( x ) † {\displaystyle J(\mathbf {x} )^{\dagger }} is the Moore-Penrose inverse (also known as pseudoinverse ) of the Jacobian matrix J ( x ) {\displaystyle J(\mathbf {x} )} of f ( x ) {\displaystyle \mathbf {f} (\mathbf {x} )}.

It can be considered an extension of Newton's method and enjoys the same local quadratic convergence [4] toward isolated regular solutions.

If the solution doesn't exist but the initial iterate x ( 0 ) {\displaystyle \mathbf {x} ^{(0)}} is near a point x ^ = ( x ^ 1 , … , x ^ n ) {\displaystyle {\hat {\mathbf {x} }}=({\hat {x}}_{1},\ldots ,{\hat {x}}_{n})} at which the sum of squares ∑ i = 1 m | f i ( x 1 , … , x n ) | 2 ≡ ‖ f ( x ) ‖ 2 2 {\textstyle \sum _{i=1}^{m}|f_{i}(x_{1},\ldots ,x_{n})|^{2}\equiv \|\mathbf {f} (\mathbf {x} )\|_{2}^{2}} reaches a small local minimum, the Gauss-Newton iteration linearly converges to x ^ {\displaystyle {\hat {\mathbf {x} }}}.

The point x ^ {\displaystyle {\hat {\mathbf {x} }}} is often called a least squares solution of the overdetermined system.

Derivation from Newton's method [ edit ] In what follows, the Gauss–Newton algorithm will be derived from Newton's method for function optimization via an approximation.

As a consequence, the rate of convergence of the Gauss–Newton algorithm can be quadratic under certain regularity conditions.

In general (under weaker conditions), the convergence rate is linear.

[9] The recurrence relation for Newton's method for minimizing a function S of parameters β {\displaystyle {\boldsymbol {\beta }}} is β ( s + 1 ) = β ( s ) − H − 1 g , {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\mathbf {H} ^{-1}\mathbf {g} ,} where g denotes the gradient vector of S , and H denotes the Hessian matrix of S.

Since S = ∑ i = 1 m r i 2 {\textstyle S=\sum _{i=1}^{m}r_{i}^{2}} , the gradient is given by g j = 2 ∑ i = 1 m r i ∂ r i ∂ β j.

{\displaystyle g_{j}=2\sum _{i=1}^{m}r_{i}{\frac {\partial r_{i}}{\partial \beta _{j}}}.

} Elements of the Hessian are calculated by differentiating the gradient elements, g j {\displaystyle g_{j}} , with respect to β k {\displaystyle \beta _{k}} : H j k = 2 ∑ i = 1 m ( ∂ r i ∂ β j ∂ r i ∂ β k + r i ∂ 2 r i ∂ β j ∂ β k ).

{\displaystyle H_{jk}=2\sum _{i=1}^{m}\left({\frac {\partial r_{i}}{\partial \beta _{j}}}{\frac {\partial r_{i}}{\partial \beta _{k}}}+r_{i}{\frac {\partial ^{2}r_{i}}{\partial \beta _{j}\partial \beta _{k}}}\right).

} The Gauss–Newton method is obtained by ignoring the second-order derivative terms (the second term in this expression).

That is, the Hessian is approximated by H j k ≈ 2 ∑ i = 1 m J i j J i k , {\displaystyle H_{jk}\approx 2\sum _{i=1}^{m}J_{ij}J_{ik},} where J i j = ∂ r i / ∂ β j {\textstyle J_{ij}={\partial r_{i}}/{\partial \beta _{j}}} are entries of the Jacobian J r.

Note that when the exact hessian is evaluated near an exact fit we have near-zero r i {\displaystyle r_{i}} , so the second term becomes near-zero as well, which justifies the approximation.

The gradient and the approximate Hessian can be written in matrix notation as g = 2 J r T r , H ≈ 2 J r T J r.

{\displaystyle \mathbf {g} =2{\mathbf {J} _{\mathbf {r} }}^{\operatorname {T} }\mathbf {r} ,\quad \mathbf {H} \approx 2{\mathbf {J} _{\mathbf {r} }}^{\operatorname {T} }\mathbf {J_{r}}.

} These expressions are substituted into the recurrence relation above to obtain the operational equations β ( s + 1 ) = β ( s ) + Δ ; Δ = − ( J r T J r ) − 1 J r T r.

{\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}+\Delta ;\quad \Delta =-\left(\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {J_{r}} \right)^{-1}\mathbf {J_{r}} ^{\operatorname {T} }\mathbf {r}.

} Convergence of the Gauss–Newton method is not guaranteed in all instances.

The approximation | r i ∂ 2 r i ∂ β j ∂ β k | ≪ | ∂ r i ∂ β j ∂ r i ∂ β k | {\displaystyle \left|r_{i}{\frac {\partial ^{2}r_{i}}{\partial \beta _{j}\partial \beta _{k}}}\right|\ll \left|{\frac {\partial r_{i}}{\partial \beta _{j}}}{\frac {\partial r_{i}}{\partial \beta _{k}}}\right|} that needs to hold to be able to ignore the second-order derivative terms may be valid in two cases, for which convergence is to be expected: [10] The function values r i {\displaystyle r_{i}} are small in magnitude, at least around the minimum.

The functions are only "mildly" nonlinear, so that ∂ 2 r i ∂ β j ∂ β k {\textstyle {\frac {\partial ^{2}r_{i}}{\partial \beta _{j}\partial \beta _{k}}}} is relatively small in magnitude.

Improved versions [ edit ] With the Gauss–Newton method the sum of squares of the residuals S may not decrease at every iteration.

However, since Δ is a descent direction, unless S ( β s ) {\displaystyle S\left({\boldsymbol {\beta }}^{s}\right)} is a stationary point, it holds that S ( β s + α Δ ) < S ( β s ) {\displaystyle S\left({\boldsymbol {\beta }}^{s}+\alpha \Delta \right)<S\left({\boldsymbol {\beta }}^{s}\right)} for all sufficiently small α > 0 {\displaystyle \alpha >0}.

Thus, if divergence occurs, one solution is to employ a fraction α {\displaystyle \alpha } of the increment vector Δ in the updating formula: β s + 1 = β s + α Δ.

{\displaystyle {\boldsymbol {\beta }}^{s+1}={\boldsymbol {\beta }}^{s}+\alpha \Delta.

} In other words, the increment vector is too long, but it still points "downhill", so going just a part of the way will decrease the objective function S.

An optimal value for α {\displaystyle \alpha } can be found by using a line search algorithm, that is, the magnitude of α {\displaystyle \alpha } is determined by finding the value that minimizes S , usually using a direct search method in the interval 0 < α < 1 {\displaystyle 0<\alpha <1} or a backtracking line search such as Armijo-line search.

Typically, α {\displaystyle \alpha } should be chosen such that it satisfies the Wolfe conditions or the Goldstein conditions.

[11] In cases where the direction of the shift vector is such that the optimal fraction α is close to zero, an alternative method for handling divergence is the use of the Levenberg–Marquardt algorithm , a trust region method.

[3] The normal equations are modified in such a way that the increment vector is rotated towards the direction of steepest descent , ( J T J + λ D ) Δ = − J T r , {\displaystyle \left(\mathbf {J^{\operatorname {T} }J+\lambda D} \right)\Delta =-\mathbf {J} ^{\operatorname {T} }\mathbf {r} ,} where D is a positive diagonal matrix.

Note that when D is the identity matrix I and λ → + ∞ {\displaystyle \lambda \to +\infty } , then λ Δ = λ ( J T J + λ I ) − 1 ( − J T r ) = ( I − J T J / λ + ⋯ ) ( − J T r ) → − J T r {\displaystyle \lambda \Delta =\lambda \left(\mathbf {J^{\operatorname {T} }J} +\lambda \mathbf {I} \right)^{-1}\left(-\mathbf {J} ^{\operatorname {T} }\mathbf {r} \right)=\left(\mathbf {I} -\mathbf {J^{\operatorname {T} }J} /\lambda +\cdots \right)\left(-\mathbf {J} ^{\operatorname {T} }\mathbf {r} \right)\to -\mathbf {J} ^{\operatorname {T} }\mathbf {r} } , therefore the direction of Δ approaches the direction of the negative gradient − J T r {\displaystyle -\mathbf {J} ^{\operatorname {T} }\mathbf {r} }.

The so-called Marquardt parameter λ {\displaystyle \lambda } may also be optimized by a line search, but this is inefficient, as the shift vector must be recalculated every time λ {\displaystyle \lambda } is changed.

A more efficient strategy is this: When divergence occurs, increase the Marquardt parameter until there is a decrease in S.

Then retain the value from one iteration to the next, but decrease it if possible until a cut-off value is reached, when the Marquardt parameter can be set to zero; the minimization of S then becomes a standard Gauss–Newton minimization.

Large-scale optimization [ edit ] For large-scale optimization, the Gauss–Newton method is of special interest because it is often (though certainly not always) true that the matrix J r {\displaystyle \mathbf {J} _{\mathbf {r} }} is more sparse than the approximate Hessian J r T J r {\displaystyle \mathbf {J} _{\mathbf {r} }^{\operatorname {T} }\mathbf {J_{r}} }.

In such cases, the step calculation itself will typically need to be done with an approximate iterative method appropriate for large and sparse problems, such as the conjugate gradient method.

In order to make this kind of approach work, one needs at least an efficient method for computing the product J r T J r p {\displaystyle {\mathbf {J} _{\mathbf {r} }}^{\operatorname {T} }\mathbf {J_{r}} \mathbf {p} } for some vector p.

With sparse matrix storage, it is in general practical to store the rows of J r {\displaystyle \mathbf {J} _{\mathbf {r} }} in a compressed form (e.

g.

, without zero entries), making a direct computation of the above product tricky due to the transposition.

However, if one defines c i as row i of the matrix J r {\displaystyle \mathbf {J} _{\mathbf {r} }} , the following simple relation holds: J r T J r p = ∑ i c i ( c i ⋅ p ) , {\displaystyle {\mathbf {J} _{\mathbf {r} }}^{\operatorname {T} }\mathbf {J_{r}} \mathbf {p} =\sum _{i}\mathbf {c} _{i}\left(\mathbf {c} _{i}\cdot \mathbf {p} \right),} so that every row contributes additively and independently to the product.

In addition to respecting a practical sparse storage structure, this expression is well suited for parallel computations.

Note that every row c i is the gradient of the corresponding residual r i ; with this in mind, the formula above emphasizes the fact that residuals contribute to the problem independently of each other.

Related algorithms [ edit ] In a quasi-Newton method , such as that due to Davidon, Fletcher and Powell or Broyden–Fletcher–Goldfarb–Shanno ( BFGS method ) an estimate of the full Hessian ∂ 2 S ∂ β j ∂ β k {\textstyle {\frac {\partial ^{2}S}{\partial \beta _{j}\partial \beta _{k}}}} is built up numerically using first derivatives ∂ r i ∂ β j {\textstyle {\frac {\partial r_{i}}{\partial \beta _{j}}}} only so that after n refinement cycles the method closely approximates to Newton's method in performance.

Note that quasi-Newton methods can minimize general real-valued functions, whereas Gauss–Newton, Levenberg–Marquardt, etc.

fits only to nonlinear least-squares problems.

Another method for solving minimization problems using only first derivatives is gradient descent.

However, this method does not take into account the second derivatives even approximately.

Consequently, it is highly inefficient for many functions, especially if the parameters have strong interactions.

Example implementations [ edit ] Julia [ edit ] The following implementation in Julia provides one method which uses a provided Jacobian and another computing with automatic differentiation.

""" gaussnewton(r,J,β₀,maxiter,tol) Perform Gauss-Newton optimization to minimize the residual function `r` with Jacobian `J` starting from `β₀`.

The algorithm terminates when the norm of the step is less than `tol` or after `maxiter` iterations.

""" function gaussnewton ( r , J , β₀ , maxiter , tol ) β = copy ( β₀ ) for _ in 1 : maxiter Jβ = J ( β ); Δ = - ( Jβ '* Jβ ) \ ( Jβ '* r ( β )) β += Δ if sqrt ( sum ( abs2 , Δ )) < tol break end end return β end import AbstractDifferentiation as AD , Zygote backend = AD.

ZygoteBackend () # other backends are available """ gaussnewton(r,β₀,maxiter,tol) Perform Gauss-Newton optimization to minimize the residual function `r` starting from `β₀`.

The relevant Jacobian is calculated using automatic differentiation.

The algorithm terminates when the norm of the step is less than `tol` or after `maxiter` iterations.

""" function gaussnewton ( r , β₀ , maxiter , tol ) β = copy ( β₀ ) for _ in 1 : maxiter rβ , Jβ = AD.

value_and_jacobian ( backend , r , β ) Δ = - ( Jβ [ 1 ] '* Jβ [ 1 ]) \ ( Jβ [ 1 ] '* rβ ) β += Δ if sqrt ( sum ( abs2 , Δ )) < tol break end end return β end Notes [ edit ] ^ Mittelhammer, Ron C.

; Miller, Douglas J.

; Judge, George G.

(2000).

Econometric Foundations.

Cambridge: Cambridge University Press.

pp.

197–198.

ISBN 0-521-62394-4.

^ Floudas, Christodoulos A.

; Pardalos, Panos M.

(2008).

Encyclopedia of Optimization.

Springer.

p.

1130.

ISBN 9780387747583.

^ a b Björck (1996) ^ a b J.

E.

Dennis, Jr.

and R.

B.

Schnabel (1983).

Numerical Methods for Unconstrained Optimization and Nonlinear Equations.

SIAM 1996 reproduction of Prentice-Hall 1983 edition.

p.

222.

^ Björck (1996), p.

260.

^ Mascarenhas (2013), "The divergence of the BFGS and Gauss Newton Methods", Mathematical Programming , 147 (1): 253–276, arXiv : 1309.

7922 , doi : 10.

1007/s10107-013-0720-6 , S2CID 14700106 ^ Björck (1996), p.

341, 342.

^ Fletcher (1987), p.

113.

^ "Archived copy" (PDF).

Archived from the original (PDF) on 2016-08-04.

Retrieved 2014-04-25.

{{ cite web }} : CS1 maint: archived copy as title ( link ) ^ Nocedal (1999), p.

259.

^ Nocedal, Jorge.

(1999).

Numerical optimization.

Wright, Stephen J.

, 1960-.

New York: Springer.

ISBN 0387227423.

OCLC 54849297.

References [ edit ] Björck, A.

(1996).

Numerical methods for least squares problems.

SIAM, Philadelphia.

ISBN 0-89871-360-9.

Fletcher, Roger (1987).

Practical methods of optimization (2nd ed.

).

New York: John Wiley & Sons.

ISBN 978-0-471-91547-8.

Nocedal, Jorge; Wright, Stephen (1999).

Numerical optimization.

New York: Springer.

ISBN 0-387-98793-2.

External links [ edit ] Probability, Statistics and Estimation The algorithm is detailed and applied to the biology experiment discussed as an example in this article (page 84 with the uncertainties on the estimated values).

Implementations [ edit ] Artelys Knitro is a non-linear solver with an implementation of the Gauss–Newton method.

It is written in C and has interfaces to C++/C#/Java/Python/MATLAB/R.

v t e Sir Isaac Newton Publications Fluxions (1671) De Motu (1684) Principia (1687) Opticks (1704) Queries (1704) Arithmetica (1707) De Analysi (1711) Other writings Quaestiones (1661–1665) " standing on the shoulders of giants " (1675) Notes on the Jewish Temple (c.

1680) " General Scholium " (1713; " hypotheses non fingo " ) Ancient Kingdoms Amended (1728) Corruptions of Scripture (1754) Contributions Calculus fluxion Impact depth Inertia Newton disc Newton polygon Newton–Okounkov body Newton's reflector Newtonian telescope Newton scale Newton's metal Spectrum Structural coloration Newtonianism Bucket argument Newton's inequalities Newton's law of cooling Newton's law of universal gravitation post-Newtonian expansion parameterized gravitational constant Newton–Cartan theory Schrödinger–Newton equation Newton's laws of motion Kepler's laws Newtonian dynamics Newton's method in optimization Apollonius's problem truncated Newton method Gauss–Newton algorithm Newton's rings Newton's theorem about ovals Newton–Pepys problem Newtonian potential Newtonian fluid Classical mechanics Corpuscular theory of light Leibniz–Newton calculus controversy Newton's notation Rotating spheres Newton's cannonball Newton–Cotes formulas Newton's method generalized Gauss–Newton method Newton fractal Newton's identities Newton polynomial Newton's theorem of revolving orbits Newton–Euler equations Newton number kissing number problem Newton's quotient Parallelogram of force Newton–Puiseux theorem Absolute space and time Luminiferous aether Newtonian series table Personal life Woolsthorpe Manor (birthplace) Cranbury Park (home) Early life Later life Apple tree Religious views Occult studies Scientific Revolution Copernican Revolution Relations Catherine Barton (niece) John Conduitt (nephew-in-law) Isaac Barrow (professor) William Clarke (mentor) Benjamin Pulleyn (tutor) John Keill (disciple) William Stukeley (friend) William Jones (friend) Abraham de Moivre (friend) Depictions Newton by Blake (monotype) Newton by Paolozzi (sculpture) Isaac Newton Gargoyle Astronomers Monument Namesake Newton (unit) Newton's cradle Isaac Newton Institute Isaac Newton Medal Isaac Newton Telescope Isaac Newton Group of Telescopes XMM-Newton Sir Isaac Newton Sixth Form Statal Institute of Higher Education Isaac Newton Newton International Fellowship Categories Isaac Newton v t e Least squares and regression analysis Computational statistics Least squares Linear least squares Non-linear least squares Iteratively reweighted least squares Correlation and dependence Pearson product-moment correlation Rank correlation ( Spearman's rho Kendall's tau ) Partial correlation Confounding variable Regression analysis Ordinary least squares Partial least squares Total least squares Ridge regression Regression as a statistical model Linear regression Simple linear regression Ordinary least squares Generalized least squares Weighted least squares General linear model Predictor structure Polynomial regression Growth curve (statistics) Segmented regression Local regression Non-standard Nonlinear regression Nonparametric Semiparametric Robust Quantile Isotonic Non-normal errors Generalized linear model Binomial Poisson Logistic Decomposition of variance Analysis of variance Analysis of covariance Multivariate AOV Model exploration Stepwise regression Model selection Mallows's C p AIC BIC Model specification Regression validation Background Mean and predicted response Gauss–Markov theorem Errors and residuals Goodness of fit Studentized residual Minimum mean-square error Frisch–Waugh–Lovell theorem Design of experiments Response surface methodology Optimal design Bayesian design Numerical approximation Numerical analysis Approximation theory Numerical integration Gaussian quadrature Orthogonal polynomials Chebyshev polynomials Chebyshev nodes Applications Curve fitting Calibration curve Numerical smoothing and differentiation System identification Moving least squares Regression analysis category Statistics category Mathematics portal Statistics outline Statistics topics v t e Optimization : Algorithms , methods , and heuristics Unconstrained nonlinear Functions Golden-section search Powell's method Line search Nelder–Mead method Successive parabolic interpolation Gradients Convergence Trust region Wolfe conditions Quasi–Newton Berndt–Hall–Hall–Hausman Broyden–Fletcher–Goldfarb–Shanno and L-BFGS Davidon–Fletcher–Powell Symmetric rank-one (SR1) Other methods Conjugate gradient Gauss–Newton Gradient Mirror Levenberg–Marquardt Powell's dog leg method Truncated Newton Hessians Newton's method Optimization computes maxima and minima.

Constrained nonlinear General Barrier methods Penalty methods Differentiable Augmented Lagrangian methods Sequential quadratic programming Successive linear programming Convex optimization Convex minimization Cutting-plane method Reduced gradient (Frank–Wolfe) Subgradient method Linear and quadratic Interior point Affine scaling Ellipsoid algorithm of Khachiyan Projective algorithm of Karmarkar Basis- exchange Simplex algorithm of Dantzig Revised simplex algorithm Criss-cross algorithm Principal pivoting algorithm of Lemke Combinatorial Paradigms Approximation algorithm Dynamic programming Greedy algorithm Integer programming Branch and bound / cut Graph algorithms Minimum spanning tree Borůvka Prim Kruskal Shortest path Bellman–Ford SPFA Dijkstra Floyd–Warshall Network flows Dinic Edmonds–Karp Ford–Fulkerson Push–relabel maximum flow Metaheuristics Evolutionary algorithm Hill climbing Local search Parallel metaheuristics Simulated annealing Spiral optimization algorithm Tabu search Software Retrieved from " https://en.

wikipedia.

org/w/index.

php?title=Gauss–Newton_algorithm&oldid=1228538455 " Categories : Optimization algorithms and methods Least squares Statistical algorithms Hidden categories: CS1 maint: archived copy as title Articles with short description Short description is different from Wikidata This page was last edited on 11 June 2024, at 19:52 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 4.

0 ; additional terms may apply.

By using this site, you agree to the Terms of Use and Privacy Policy.

Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view.