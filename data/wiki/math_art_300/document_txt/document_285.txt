Jacobian matrix and determinant - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Donate Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Create account Log in Personal tools Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Definition 2 Jacobian matrix 3 Jacobian determinant 4 Inverse 5 Critical points 6 Examples Toggle Examples subsection 6.

1 Example 1 6.

2 Example 2: polar-Cartesian transformation 6.

3 Example 3: spherical-Cartesian transformation 6.

4 Example 4 6.

5 Example 5 7 Other uses Toggle Other uses subsection 7.

1 Dynamical systems 7.

2 Newton's method 7.

3 Regression and least squares fitting 8 See also 9 Notes 10 References 11 Further reading 12 External links Toggle the table of contents Jacobian matrix and determinant 32 languages العربية Беларуская Català Čeština Deutsch Ελληνικά Español Esperanto فارسی Français 한국어 Bahasa Indonesia Íslenska Italiano עברית ಕನ್ನಡ Magyar Nederlands 日本語 Norsk bokmål Polski Português Русский Shqip Slovenščina Српски / srpski Suomi Svenska Türkçe Українська Tiếng Việt 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version Appearance move to sidebar hide From Wikipedia, the free encyclopedia Matrix of all first-order partial derivatives of a vector-valued function "Jacobian matrix" redirects here.

For the operator, see Jacobi matrix (operator).

Part of a series of articles about Calculus ∫ a b f ′ ( t ) d t = f ( b ) − f ( a ) {\displaystyle \int _{a}^{b}f'(t)\,dt=f(b)-f(a)} Fundamental theorem Limits Continuity Rolle's theorem Mean value theorem Inverse function theorem Differential Definitions Derivative ( generalizations ) Differential infinitesimal of a function total Concepts Differentiation notation Second derivative Implicit differentiation Logarithmic differentiation Related rates Taylor's theorem Rules and identities Sum Product Chain Power Quotient L'Hôpital's rule Inverse General Leibniz Faà di Bruno's formula Reynolds Integral Lists of integrals Integral transform Leibniz integral rule Definitions Antiderivative Integral ( improper ) Riemann integral Lebesgue integration Contour integration Integral of inverse functions Integration by Parts Discs Cylindrical shells Substitution ( trigonometric , tangent half-angle , Euler ) Euler's formula Partial fractions Changing order Reduction formulae Differentiating under the integral sign Risch algorithm Series Geometric ( arithmetico-geometric ) Harmonic Alternating Power Binomial Taylor Convergence tests Summand limit (term test) Ratio Root Integral Direct comparison Limit comparison Alternating series Cauchy condensation Dirichlet Abel Vector Gradient Divergence Curl Laplacian Directional derivative Identities Theorems Gradient Green's Stokes' Divergence generalized Stokes Helmholtz decomposition Multivariable Formalisms Matrix Tensor Exterior Geometric Definitions Partial derivative Multiple integral Line integral Surface integral Volume integral Jacobian Hessian Advanced Calculus on Euclidean space Generalized functions Limit of distributions Specialized Fractional Malliavin Stochastic Variations Miscellaneous Precalculus History Glossary List of topics Integration Bee Mathematical analysis Nonstandard analysis v t e In vector calculus , the Jacobian matrix ( / dʒ ə ˈ k oʊ b i ə n / , [1] [2] [3] / dʒ ɪ -, j ɪ -/ ) of a vector-valued function of several variables is the matrix of all its first-order partial derivatives.

When this matrix is square , that is, when the function takes the same number of variables as input as the number of vector components of its output, its determinant is referred to as the Jacobian determinant.

Both the matrix and (if applicable) the determinant are often referred to simply as the Jacobian in literature.

[4] Definition [ edit ] Suppose f : R n → R m is a function such that each of its first-order partial derivatives exists on R n.

This function takes a point x ∈ R n as input and produces the vector f ( x ) ∈ R m as output.

Then the Jacobian matrix of f is defined to be an m × n matrix, denoted by J , whose ( i , j ) th entry is J i j = ∂ f i ∂ x j {\textstyle \mathbf {J} _{ij}={\frac {\partial f_{i}}{\partial x_{j}}}} , or explicitly J = [ ∂ f ∂ x 1 ⋯ ∂ f ∂ x n ] = [ ∇ T f 1 ⋮ ∇ T f m ] = [ ∂ f 1 ∂ x 1 ⋯ ∂ f 1 ∂ x n ⋮ ⋱ ⋮ ∂ f m ∂ x 1 ⋯ ∂ f m ∂ x n ] {\displaystyle \mathbf {J} ={\begin{bmatrix}{\dfrac {\partial \mathbf {f} }{\partial x_{1}}}&\cdots &{\dfrac {\partial \mathbf {f} }{\partial x_{n}}}\end{bmatrix}}={\begin{bmatrix}\nabla ^{\mathrm {T} }f_{1}\\\vdots \\\nabla ^{\mathrm {T} }f_{m}\end{bmatrix}}={\begin{bmatrix}{\dfrac {\partial f_{1}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{1}}{\partial x_{n}}}\\\vdots &\ddots &\vdots \\{\dfrac {\partial f_{m}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{m}}{\partial x_{n}}}\end{bmatrix}}} where ∇ T f i {\displaystyle \nabla ^{\mathrm {T} }f_{i}} is the transpose (row vector) of the gradient of the i {\displaystyle i} -th component.

The Jacobian matrix, whose entries are functions of x , is denoted in various ways; common notations include D f , J f , ∇ f {\displaystyle \nabla \mathbf {f} } , and ∂ ( f 1 ,.

, f m ) ∂ ( x 1 ,.

, x n ) {\displaystyle {\frac {\partial (f_{1},.

,f_{m})}{\partial (x_{1},.

,x_{n})}}}.

[5] [6] Some authors define the Jacobian as the transpose of the form given above.

The Jacobian matrix represents the differential of f at every point where f is differentiable.

In detail, if h is a displacement vector represented by a column matrix , the matrix product J ( x ) ⋅ h is another displacement vector, that is the best linear approximation of the change of f in a neighborhood of x , if f ( x ) is differentiable at x.

[a] This means that the function that maps y to f ( x ) + J ( x ) ⋅ ( y – x ) is the best linear approximation of f ( y ) for all points y close to x.

The linear map h → J ( x ) ⋅ h is known as the derivative or the differential of f at x.

When m = n , the Jacobian matrix is square, so its determinant is a well-defined function of x , known as the Jacobian determinant of f.

It carries important information about the local behavior of f.

In particular, the function f has a differentiable inverse function in a neighborhood of a point x if and only if the Jacobian determinant is nonzero at x (see Jacobian conjecture for a related problem of global invertibility).

The Jacobian determinant also appears when changing the variables in multiple integrals (see substitution rule for multiple variables ).

When m = 1 , that is when f : R n → R is a scalar-valued function , the Jacobian matrix reduces to the row vector ∇ T f {\displaystyle \nabla ^{\mathrm {T} }f} ; this row vector of all first-order partial derivatives of f is the transpose of the gradient of f , i.

e.

J f = ∇ T f {\displaystyle \mathbf {J} _{f}=\nabla ^{T}f}.

Specializing further, when m = n = 1 , that is when f : R → R is a scalar-valued function of a single variable, the Jacobian matrix has a single entry; this entry is the derivative of the function f.

These concepts are named after the mathematician Carl Gustav Jacob Jacobi (1804–1851).

Jacobian matrix [ edit ] The Jacobian of a vector-valued function in several variables generalizes the gradient of a scalar -valued function in several variables, which in turn generalizes the derivative of a scalar-valued function of a single variable.

In other words, the Jacobian matrix of a scalar-valued function in several variables is (the transpose of) its gradient and the gradient of a scalar-valued function of a single variable is its derivative.

At each point where a function is differentiable, its Jacobian matrix can also be thought of as describing the amount of "stretching", "rotating" or "transforming" that the function imposes locally near that point.

For example, if ( x ′, y ′) = f ( x , y ) is used to smoothly transform an image, the Jacobian matrix J f ( x , y ) , describes how the image in the neighborhood of ( x , y ) is transformed.

If a function is differentiable at a point, its differential is given in coordinates by the Jacobian matrix.

However a function does not need to be differentiable for its Jacobian matrix to be defined, since only its first-order partial derivatives are required to exist.

If f is differentiable at a point p in R n , then its differential is represented by J f ( p ).

In this case, the linear transformation represented by J f ( p ) is the best linear approximation of f near the point p , in the sense that f ( x ) − f ( p ) = J f ( p ) ( x − p ) + o ( ‖ x − p ‖ ) ( as x → p ) , {\displaystyle \mathbf {f} (\mathbf {x} )-\mathbf {f} (\mathbf {p} )=\mathbf {J} _{\mathbf {f} }(\mathbf {p} )(\mathbf {x} -\mathbf {p} )+o(\|\mathbf {x} -\mathbf {p} \|)\quad ({\text{as }}\mathbf {x} \to \mathbf {p} ),} where o (‖ x − p ‖) is a quantity that approaches zero much faster than the distance between x and p does as x approaches p.

This approximation specializes to the approximation of a scalar function of a single variable by its Taylor polynomial of degree one, namely f ( x ) − f ( p ) = f ′ ( p ) ( x − p ) + o ( x − p ) ( as x → p ).

{\displaystyle f(x)-f(p)=f'(p)(x-p)+o(x-p)\quad ({\text{as }}x\to p).

} In this sense, the Jacobian may be regarded as a kind of " first-order derivative " of a vector-valued function of several variables.

In particular, this means that the gradient of a scalar-valued function of several variables may too be regarded as its "first-order derivative".

Composable differentiable functions f : R n → R m and g : R m → R k satisfy the chain rule , namely J g ∘ f ( x ) = J g ( f ( x ) ) J f ( x ) {\displaystyle \mathbf {J} _{\mathbf {g} \circ \mathbf {f} }(\mathbf {x} )=\mathbf {J} _{\mathbf {g} }(\mathbf {f} (\mathbf {x} ))\mathbf {J} _{\mathbf {f} }(\mathbf {x} )} for x in R n.

The Jacobian of the gradient of a scalar function of several variables has a special name: the Hessian matrix , which in a sense is the " second derivative " of the function in question.

Jacobian determinant [ edit ] A nonlinear map f : R 2 → R 2 {\displaystyle f\colon \mathbb {R} ^{2}\to \mathbb {R} ^{2}} sends a small square (left, in red) to a distorted parallelogram (right, in red).

The Jacobian at a point gives the best linear approximation of the distorted parallelogram near that point (right, in translucent white), and the Jacobian determinant gives the ratio of the area of the approximating parallelogram to that of the original square.

If m = n , then f is a function from R n to itself and the Jacobian matrix is a square matrix.

We can then form its determinant , known as the Jacobian determinant.

The Jacobian determinant is sometimes simply referred to as "the Jacobian".

The Jacobian determinant at a given point gives important information about the behavior of f near that point.

For instance, the continuously differentiable function f is invertible near a point p ∈ R n if the Jacobian determinant at p is non-zero.

This is the inverse function theorem.

Furthermore, if the Jacobian determinant at p is positive , then f preserves orientation near p ; if it is negative , f reverses orientation.

The absolute value of the Jacobian determinant at p gives us the factor by which the function f expands or shrinks volumes near p ; this is why it occurs in the general substitution rule.

The Jacobian determinant is used when making a change of variables when evaluating a multiple integral of a function over a region within its domain.

To accommodate for the change of coordinates the magnitude of the Jacobian determinant arises as a multiplicative factor within the integral.

This is because the n -dimensional dV element is in general a parallelepiped in the new coordinate system, and the n -volume of a parallelepiped is the determinant of its edge vectors.

The Jacobian can also be used to determine the stability of equilibria for systems of differential equations by approximating behavior near an equilibrium point.

Inverse [ edit ] According to the inverse function theorem , the matrix inverse of the Jacobian matrix of an invertible function is the Jacobian matrix of the inverse function.

That is, if the Jacobian of the function f : R n → R n is continuous and nonsingular at the point p in R n , then f is invertible when restricted to some neighborhood of p and J f − 1 ( x ) = 1 J f ( f − 1 ( x ) ).

{\displaystyle \mathbf {J} _{f^{-1}}(x)={\frac {1}{\mathbf {J} _{f}(f^{-1}(x))}}.

} In other words, if the Jacobian determinant is not zero at a point, then the function is locally invertible near this point, that is, there is a neighbourhood of this point in which the function is invertible.

The (unproved) Jacobian conjecture is related to global invertibility in the case of a polynomial function, that is a function defined by n polynomials in n variables.

It asserts that, if the Jacobian determinant is a non-zero constant (or, equivalently, that it does not have any complex zero), then the function is invertible and its inverse is a polynomial function.

Critical points [ edit ] Main article: Critical point If f : R n → R m is a differentiable function , a critical point of f is a point where the rank of the Jacobian matrix is not maximal.

This means that the rank at the critical point is lower than the rank at some neighbour point.

In other words, let k be the maximal dimension of the open balls contained in the image of f ; then a point is critical if all minors of rank k of f are zero.

In the case where m = n = k , a point is critical if the Jacobian determinant is zero.

Examples [ edit ] Example 1 [ edit ] Consider the function f : R 2 → R 2 , with ( x , y ) ↦ ( f 1 ( x , y ), f 2 ( x , y )), given by f ( [ x y ] ) = [ f 1 ( x , y ) f 2 ( x , y ) ] = [ x 2 y 5 x + sin ⁡ y ].

{\displaystyle \mathbf {f} \left({\begin{bmatrix}x\\y\end{bmatrix}}\right)={\begin{bmatrix}f_{1}(x,y)\\f_{2}(x,y)\end{bmatrix}}={\begin{bmatrix}x^{2}y\\5x+\sin y\end{bmatrix}}.

} Then we have f 1 ( x , y ) = x 2 y {\displaystyle f_{1}(x,y)=x^{2}y} and f 2 ( x , y ) = 5 x + sin ⁡ y {\displaystyle f_{2}(x,y)=5x+\sin y} and the Jacobian matrix of f is J f ( x , y ) = [ ∂ f 1 ∂ x ∂ f 1 ∂ y ∂ f 2 ∂ x ∂ f 2 ∂ y ] = [ 2 x y x 2 5 cos ⁡ y ] {\displaystyle \mathbf {J} _{\mathbf {f} }(x,y)={\begin{bmatrix}{\dfrac {\partial f_{1}}{\partial x}}&{\dfrac {\partial f_{1}}{\partial y}}\\[1em]{\dfrac {\partial f_{2}}{\partial x}}&{\dfrac {\partial f_{2}}{\partial y}}\end{bmatrix}}={\begin{bmatrix}2xy&x^{2}\\5&\cos y\end{bmatrix}}} and the Jacobian determinant is det ( J f ( x , y ) ) = 2 x y cos ⁡ y − 5 x 2.

{\displaystyle \det(\mathbf {J} _{\mathbf {f} }(x,y))=2xy\cos y-5x^{2}.

} Example 2: polar-Cartesian transformation [ edit ] The transformation from polar coordinates ( r , φ ) to Cartesian coordinates ( x , y ), is given by the function F : R + × [0, 2 π ) → R 2 with components: x = r cos ⁡ φ ; y = r sin ⁡ φ.

{\displaystyle {\begin{aligned}x&=r\cos \varphi ;\\y&=r\sin \varphi.

\end{aligned}}} J F ( r , φ ) = [ ∂ x ∂ r ∂ x ∂ φ ∂ y ∂ r ∂ y ∂ φ ] = [ cos ⁡ φ − r sin ⁡ φ sin ⁡ φ r cos ⁡ φ ] {\displaystyle \mathbf {J} _{\mathbf {F} }(r,\varphi )={\begin{bmatrix}{\frac {\partial x}{\partial r}}&{\frac {\partial x}{\partial \varphi }}\\[0.

5ex]{\frac {\partial y}{\partial r}}&{\frac {\partial y}{\partial \varphi }}\end{bmatrix}}={\begin{bmatrix}\cos \varphi &-r\sin \varphi \\\sin \varphi &r\cos \varphi \end{bmatrix}}} The Jacobian determinant is equal to r.

This can be used to transform integrals between the two coordinate systems: ∬ F ( A ) f ( x , y ) d x d y = ∬ A f ( r cos ⁡ φ , r sin ⁡ φ ) r d r d φ.

{\displaystyle \iint _{\mathbf {F} (A)}f(x,y)\,dx\,dy=\iint _{A}f(r\cos \varphi ,r\sin \varphi )\,r\,dr\,d\varphi.

} Example 3: spherical-Cartesian transformation [ edit ] The transformation from spherical coordinates ( ρ , φ , θ ) [7] to Cartesian coordinates ( x , y , z ), is given by the function F : R + × [0, π ) × [0, 2 π ) → R 3 with components: x = ρ sin ⁡ φ cos ⁡ θ ; y = ρ sin ⁡ φ sin ⁡ θ ; z = ρ cos ⁡ φ.

{\displaystyle {\begin{aligned}x&=\rho \sin \varphi \cos \theta ;\\y&=\rho \sin \varphi \sin \theta ;\\z&=\rho \cos \varphi.

\end{aligned}}} The Jacobian matrix for this coordinate change is J F ( ρ , φ , θ ) = [ ∂ x ∂ ρ ∂ x ∂ φ ∂ x ∂ θ ∂ y ∂ ρ ∂ y ∂ φ ∂ y ∂ θ ∂ z ∂ ρ ∂ z ∂ φ ∂ z ∂ θ ] = [ sin ⁡ φ cos ⁡ θ ρ cos ⁡ φ cos ⁡ θ − ρ sin ⁡ φ sin ⁡ θ sin ⁡ φ sin ⁡ θ ρ cos ⁡ φ sin ⁡ θ ρ sin ⁡ φ cos ⁡ θ cos ⁡ φ − ρ sin ⁡ φ 0 ].

{\displaystyle \mathbf {J} _{\mathbf {F} }(\rho ,\varphi ,\theta )={\begin{bmatrix}{\dfrac {\partial x}{\partial \rho }}&{\dfrac {\partial x}{\partial \varphi }}&{\dfrac {\partial x}{\partial \theta }}\\[1em]{\dfrac {\partial y}{\partial \rho }}&{\dfrac {\partial y}{\partial \varphi }}&{\dfrac {\partial y}{\partial \theta }}\\[1em]{\dfrac {\partial z}{\partial \rho }}&{\dfrac {\partial z}{\partial \varphi }}&{\dfrac {\partial z}{\partial \theta }}\end{bmatrix}}={\begin{bmatrix}\sin \varphi \cos \theta &\rho \cos \varphi \cos \theta &-\rho \sin \varphi \sin \theta \\\sin \varphi \sin \theta &\rho \cos \varphi \sin \theta &\rho \sin \varphi \cos \theta \\\cos \varphi &-\rho \sin \varphi &0\end{bmatrix}}.

} The determinant is ρ 2 sin φ.

Since dV = dx dy dz is the volume for a rectangular differential volume element (because the volume of a rectangular prism is the product of its sides), we can interpret dV = ρ 2 sin φ dρ dφ dθ as the volume of the spherical differential volume element.

Unlike rectangular differential volume element's volume, this differential volume element's volume is not a constant, and varies with coordinates ( ρ and φ ).

It can be used to transform integrals between the two coordinate systems: ∭ F ( U ) f ( x , y , z ) d x d y d z = ∭ U f ( ρ sin ⁡ φ cos ⁡ θ , ρ sin ⁡ φ sin ⁡ θ , ρ cos ⁡ φ ) ρ 2 sin ⁡ φ d ρ d φ d θ.

{\displaystyle \iiint _{\mathbf {F} (U)}f(x,y,z)\,dx\,dy\,dz=\iiint _{U}f(\rho \sin \varphi \cos \theta ,\rho \sin \varphi \sin \theta ,\rho \cos \varphi )\,\rho ^{2}\sin \varphi \,d\rho \,d\varphi \,d\theta.

} Example 4 [ edit ] The Jacobian matrix of the function F : R 3 → R 4 with components y 1 = x 1 y 2 = 5 x 3 y 3 = 4 x 2 2 − 2 x 3 y 4 = x 3 sin ⁡ x 1 {\displaystyle {\begin{aligned}y_{1}&=x_{1}\\y_{2}&=5x_{3}\\y_{3}&=4x_{2}^{2}-2x_{3}\\y_{4}&=x_{3}\sin x_{1}\end{aligned}}} is J F ( x 1 , x 2 , x 3 ) = [ ∂ y 1 ∂ x 1 ∂ y 1 ∂ x 2 ∂ y 1 ∂ x 3 ∂ y 2 ∂ x 1 ∂ y 2 ∂ x 2 ∂ y 2 ∂ x 3 ∂ y 3 ∂ x 1 ∂ y 3 ∂ x 2 ∂ y 3 ∂ x 3 ∂ y 4 ∂ x 1 ∂ y 4 ∂ x 2 ∂ y 4 ∂ x 3 ] = [ 1 0 0 0 0 5 0 8 x 2 − 2 x 3 cos ⁡ x 1 0 sin ⁡ x 1 ].

{\displaystyle \mathbf {J} _{\mathbf {F} }(x_{1},x_{2},x_{3})={\begin{bmatrix}{\dfrac {\partial y_{1}}{\partial x_{1}}}&{\dfrac {\partial y_{1}}{\partial x_{2}}}&{\dfrac {\partial y_{1}}{\partial x_{3}}}\\[1em]{\dfrac {\partial y_{2}}{\partial x_{1}}}&{\dfrac {\partial y_{2}}{\partial x_{2}}}&{\dfrac {\partial y_{2}}{\partial x_{3}}}\\[1em]{\dfrac {\partial y_{3}}{\partial x_{1}}}&{\dfrac {\partial y_{3}}{\partial x_{2}}}&{\dfrac {\partial y_{3}}{\partial x_{3}}}\\[1em]{\dfrac {\partial y_{4}}{\partial x_{1}}}&{\dfrac {\partial y_{4}}{\partial x_{2}}}&{\dfrac {\partial y_{4}}{\partial x_{3}}}\end{bmatrix}}={\begin{bmatrix}1&0&0\\0&0&5\\0&8x_{2}&-2\\x_{3}\cos x_{1}&0&\sin x_{1}\end{bmatrix}}.

} This example shows that the Jacobian matrix need not be a square matrix.

Example 5 [ edit ] The Jacobian determinant of the function F : R 3 → R 3 with components y 1 = 5 x 2 y 2 = 4 x 1 2 − 2 sin ⁡ ( x 2 x 3 ) y 3 = x 2 x 3 {\displaystyle {\begin{aligned}y_{1}&=5x_{2}\\y_{2}&=4x_{1}^{2}-2\sin(x_{2}x_{3})\\y_{3}&=x_{2}x_{3}\end{aligned}}} is | 0 5 0 8 x 1 − 2 x 3 cos ⁡ ( x 2 x 3 ) − 2 x 2 cos ⁡ ( x 2 x 3 ) 0 x 3 x 2 | = − 8 x 1 | 5 0 x 3 x 2 | = − 40 x 1 x 2.

{\displaystyle {\begin{vmatrix}0&5&0\\8x_{1}&-2x_{3}\cos(x_{2}x_{3})&-2x_{2}\cos(x_{2}x_{3})\\0&x_{3}&x_{2}\end{vmatrix}}=-8x_{1}{\begin{vmatrix}5&0\\x_{3}&x_{2}\end{vmatrix}}=-40x_{1}x_{2}.

} From this we see that F reverses orientation near those points where x 1 and x 2 have the same sign; the function is locally invertible everywhere except near points where x 1 = 0 or x 2 = 0.

Intuitively, if one starts with a tiny object around the point (1, 2, 3) and apply F to that object, one will get a resulting object with approximately 40 × 1 × 2 = 80 times the volume of the original one, with orientation reversed.

Other uses [ edit ] Dynamical systems [ edit ] Consider a dynamical system of the form x ˙ = F ( x ) {\displaystyle {\dot {\mathbf {x} }}=F(\mathbf {x} )} , where x ˙ {\displaystyle {\dot {\mathbf {x} }}} is the (component-wise) derivative of x {\displaystyle \mathbf {x} } with respect to the evolution parameter t {\displaystyle t} (time), and F : R n → R n {\displaystyle F\colon \mathbb {R} ^{n}\to \mathbb {R} ^{n}} is differentiable.

If F ( x 0 ) = 0 {\displaystyle F(\mathbf {x} _{0})=0} , then x 0 {\displaystyle \mathbf {x} _{0}} is a stationary point (also called a steady state ).

By the Hartman–Grobman theorem , the behavior of the system near a stationary point is related to the eigenvalues of J F ( x 0 ) {\displaystyle \mathbf {J} _{F}\left(\mathbf {x} _{0}\right)} , the Jacobian of F {\displaystyle F} at the stationary point.

[8] Specifically, if the eigenvalues all have real parts that are negative, then the system is stable near the stationary point.

If any eigenvalue has a real part that is positive, then the point is unstable.

If the largest real part of the eigenvalues is zero, the Jacobian matrix does not allow for an evaluation of the stability.

[9] Newton's method [ edit ] A square system of coupled nonlinear equations can be solved iteratively by Newton's method.

This method uses the Jacobian matrix of the system of equations.

Regression and least squares fitting [ edit ] The Jacobian serves as a linearized design matrix in statistical regression and curve fitting ; see non-linear least squares.

The Jacobian is also used in random matrices, moments, local sensitivity and statistical diagnostics.

[10] [11] See also [ edit ] Center manifold Hessian matrix Pushforward (differential) Notes [ edit ] ^ Differentiability at x implies, but is not implied by, the existence of all first-order partial derivatives at x , and hence is a stronger condition.

References [ edit ] ^ "Jacobian - Definition of Jacobian in English by Oxford Dictionaries".

Oxford Dictionaries - English.

Archived from the original on 1 December 2017.

Retrieved 2 May 2018.

^ "the definition of jacobian".

Dictionary.

com.

Archived from the original on 1 December 2017.

Retrieved 2 May 2018.

^ Team, Forvo.

"Jacobian pronunciation: How to pronounce Jacobian in English".

forvo.

com.

Retrieved 2 May 2018.

^ W.

, Weisstein, Eric.

"Jacobian".

mathworld.

wolfram.

com.

Archived from the original on 3 November 2017.

Retrieved 2 May 2018.

{{ cite web }} : CS1 maint: multiple names: authors list ( link ) ^ Holder, Allen; Eichholz, Joseph (2019).

An Introduction to computational science.

International Series in Operations Research & Management Science.

Cham, Switzerland: Springer.

p.

53.

ISBN 978-3-030-15679-4.

^ Lovett, Stephen (2019-12-16).

Differential Geometry of Manifolds.

CRC Press.

p.

16.

ISBN 978-0-429-60782-0.

^ Joel Hass, Christopher Heil, and Maurice Weir.

Thomas' Calculus Early Transcendentals, 14e.

Pearson, 2018, p.

959.

^ Arrowsmith, D.

K.

; Place, C.

M.

(1992).

"The Linearization Theorem".

Dynamical Systems: Differential Equations, Maps, and Chaotic Behaviour.

London: Chapman & Hall.

pp.

77–81.

ISBN 0-412-39080-9.

^ Hirsch, Morris; Smale, Stephen (1974).

Differential Equations, Dynamical Systems and Linear Algebra.

ISBN 0-12-349550-4.

^ Liu, Shuangzhe; Leiva, Victor; Zhuang, Dan; Ma, Tiefeng; Figueroa-Zúñiga, Jorge I.

(March 2022).

"Matrix differential calculus with applications in the multivariate linear model and its diagnostics".

Journal of Multivariate Analysis.

188 : 104849.

doi : 10.

1016/j.

jmva.

2021.

104849.

^ Liu, Shuangzhe; Trenkler, Götz; Kollo, Tõnu; von Rosen, Dietrich; Baksalary, Oskar Maria (2023).

"Professor Heinz Neudecker and matrix differential calculus".

Statistical Papers.

doi : 10.

1007/s00362-023-01499-w.

S2CID 263661094.

Further reading [ edit ] Gandolfo, Giancarlo (1996).

"Comparative Statics and the Correspondence Principle".

Economic Dynamics (Third ed.

).

Berlin: Springer.

pp.

305–330.

ISBN 3-540-60988-1.

Protter, Murray H.

; Morrey, Charles B.

Jr.

(1985).

"Transformations and Jacobians".

Intermediate Calculus (Second ed.

).

New York: Springer.

pp.

412–420.

ISBN 0-387-96058-9.

External links [ edit ] "Jacobian" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Mathworld A more technical explanation of Jacobians v t e Matrix classes Explicitly constrained entries Alternant Anti-diagonal Anti-Hermitian Anti-symmetric Arrowhead Band Bidiagonal Bisymmetric Block-diagonal Block Block tridiagonal Boolean Cauchy Centrosymmetric Conference Complex Hadamard Copositive Diagonally dominant Diagonal Discrete Fourier Transform Elementary Equivalent Frobenius Generalized permutation Hadamard Hankel Hermitian Hessenberg Hollow Integer Logical Matrix unit Metzler Moore Nonnegative Pentadiagonal Permutation Persymmetric Polynomial Quaternionic Signature Skew-Hermitian Skew-symmetric Skyline Sparse Sylvester Symmetric Toeplitz Triangular Tridiagonal Vandermonde Walsh Z Constant Exchange Hilbert Identity Lehmer Of ones Pascal Pauli Redheffer Shift Zero Conditions on eigenvalues or eigenvectors Companion Convergent Defective Definite Diagonalizable Hurwitz Positive-definite Stieltjes Satisfying conditions on products or inverses Congruent Idempotent or Projection Invertible Involutory Nilpotent Normal Orthogonal Unimodular Unipotent Unitary Totally unimodular Weighing With specific applications Adjugate Alternating sign Augmented Bézout Carleman Cartan Circulant Cofactor Commutation Confusion Coxeter Distance Duplication and elimination Euclidean distance Fundamental (linear differential equation) Generator Gram Hessian Householder Jacobian Moment Payoff Pick Random Rotation Seifert Shear Similarity Symplectic Totally positive Transformation Used in statistics Centering Correlation Covariance Design Doubly stochastic Fisher information Hat Precision Stochastic Transition Used in graph theory Adjacency Biadjacency Degree Edmonds Incidence Laplacian Seidel adjacency Tutte Used in science and engineering Cabibbo–Kobayashi–Maskawa Density Fundamental (computer vision) Fuzzy associative Gamma Gell-Mann Hamiltonian Irregular Overlap S State transition Substitution Z (chemistry) Related terms Jordan normal form Linear independence Matrix exponential Matrix representation of conic sections Perfect matrix Pseudoinverse Row echelon form Wronskian Mathematics portal List of matrices Category:Matrices Retrieved from " https://en.

wikipedia.

org/w/index.

php?title=Jacobian_matrix_and_determinant&oldid=1234301948 " Categories : Multivariable calculus Differential calculus Generalizations of the derivative Determinants Matrices Differential operators Hidden categories: CS1 maint: multiple names: authors list Articles with short description Short description is different from Wikidata Pages using sidebar with the child parameter This page was last edited on 13 July 2024, at 16:53 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 4.

0 ; additional terms may apply.

By using this site, you agree to the Terms of Use and Privacy Policy.

Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view.