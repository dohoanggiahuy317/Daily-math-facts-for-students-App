Inverse function theorem - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Donate Contribute Help Learn to edit Community portal Recent changes Upload file Search Search Appearance Create account Log in Personal tools Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Statements 2 Example 3 Counter-example 4 Methods of proof Toggle Methods of proof subsection 4.

1 A proof using successive approximation 4.

2 A proof using the contraction mapping principle 5 Applications Toggle Applications subsection 5.

1 Implicit function theorem 5.

2 Giving a manifold structure 6 Global version 7 Holomorphic inverse function theorem 8 Formulations for manifolds 9 Generalizations Toggle Generalizations subsection 9.

1 Banach spaces 9.

2 Constant rank theorem 9.

3 Polynomial functions 9.

4 Selections 10 See also 11 Notes 12 References Toggle the table of contents Inverse function theorem 20 languages Català Čeština Deutsch Español Français Galego 한국어 Italiano עברית Magyar 日本語 Norsk bokmål Piemontèis Português Русский Svenska ไทย Türkçe Українська 中文 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version Appearance move to sidebar hide From Wikipedia, the free encyclopedia Theorem in mathematics Part of a series of articles about Calculus ∫ a b f ′ ( t ) d t = f ( b ) − f ( a ) {\displaystyle \int _{a}^{b}f'(t)\,dt=f(b)-f(a)} Fundamental theorem Limits Continuity Rolle's theorem Mean value theorem Inverse function theorem Differential Definitions Derivative ( generalizations ) Differential infinitesimal of a function total Concepts Differentiation notation Second derivative Implicit differentiation Logarithmic differentiation Related rates Taylor's theorem Rules and identities Sum Product Chain Power Quotient L'Hôpital's rule Inverse General Leibniz Faà di Bruno's formula Reynolds Integral Lists of integrals Integral transform Leibniz integral rule Definitions Antiderivative Integral ( improper ) Riemann integral Lebesgue integration Contour integration Integral of inverse functions Integration by Parts Discs Cylindrical shells Substitution ( trigonometric , tangent half-angle , Euler ) Euler's formula Partial fractions Changing order Reduction formulae Differentiating under the integral sign Risch algorithm Series Geometric ( arithmetico-geometric ) Harmonic Alternating Power Binomial Taylor Convergence tests Summand limit (term test) Ratio Root Integral Direct comparison Limit comparison Alternating series Cauchy condensation Dirichlet Abel Vector Gradient Divergence Curl Laplacian Directional derivative Identities Theorems Gradient Green's Stokes' Divergence generalized Stokes Helmholtz decomposition Multivariable Formalisms Matrix Tensor Exterior Geometric Definitions Partial derivative Multiple integral Line integral Surface integral Volume integral Jacobian Hessian Advanced Calculus on Euclidean space Generalized functions Limit of distributions Specialized Fractional Malliavin Stochastic Variations Miscellaneous Precalculus History Glossary List of topics Integration Bee Mathematical analysis Nonstandard analysis v t e In mathematics , specifically differential calculus , the inverse function theorem gives a sufficient condition for a function to be invertible in a neighborhood of a point in its domain : namely, that its derivative is continuous and non-zero at the point.

The theorem also gives a formula for the derivative of the inverse function.

In multivariable calculus , this theorem can be generalized to any continuously differentiable , vector-valued function whose Jacobian determinant is nonzero at a point in its domain, giving a formula for the Jacobian matrix of the inverse.

There are also versions of the inverse function theorem for complex holomorphic functions , for differentiable maps between manifolds , for differentiable functions between Banach spaces , and so forth.

The theorem was first established by Picard and Goursat using an iterative scheme: the basic idea is to prove a fixed point theorem using the contraction mapping theorem.

Statements [ edit ] For functions of a single variable , the theorem states that if f {\displaystyle f} is a continuously differentiable function with nonzero derivative at the point a {\displaystyle a} ; then f {\displaystyle f} is injective (or bijective onto the image) in a neighborhood of a {\displaystyle a} , the inverse is continuously differentiable near b = f ( a ) {\displaystyle b=f(a)} , and the derivative of the inverse function at b {\displaystyle b} is the reciprocal of the derivative of f {\displaystyle f} at a {\displaystyle a} : ( f − 1 ) ′ ( b ) = 1 f ′ ( a ) = 1 f ′ ( f − 1 ( b ) ).

{\displaystyle {\bigl (}f^{-1}{\bigr )}'(b)={\frac {1}{f'(a)}}={\frac {1}{f'(f^{-1}(b))}}.

} It can happen that a function f {\displaystyle f} may be injective near a point a {\displaystyle a} while f ′ ( a ) = 0 {\displaystyle f'(a)=0}.

An example is f ( x ) = ( x − a ) 3 {\displaystyle f(x)=(x-a)^{3}}.

In fact, for such a function, the inverse cannot be differentiable at b = f ( a ) {\displaystyle b=f(a)} , since if f − 1 {\displaystyle f^{-1}} were differentiable at b {\displaystyle b} , then, by the chain rule, 1 = ( f − 1 ∘ f ) ′ ( a ) = ( f − 1 ) ′ ( b ) f ′ ( a ) {\displaystyle 1=(f^{-1}\circ f)'(a)=(f^{-1})'(b)f'(a)} , which implies f ′ ( a ) ≠ 0 {\displaystyle f'(a)\neq 0}.

(The situation is different for holomorphic functions; see #Holomorphic inverse function theorem below.

) For functions of more than one variable, the theorem states that if f {\displaystyle f} is a continuously differentiable function from an open subset A {\displaystyle A} of R n {\displaystyle \mathbb {R} ^{n}} into R n {\displaystyle \mathbb {R} ^{n}} , and the derivative f ′ ( a ) {\displaystyle f'(a)} is invertible at a point a (that is, the determinant of the Jacobian matrix of f at a is non-zero), then there exist neighborhoods U {\displaystyle U} of a {\displaystyle a} in A {\displaystyle A} and V {\displaystyle V} of b = f ( a ) {\displaystyle b=f(a)} such that f ( U ) ⊂ V {\displaystyle f(U)\subset V} and f : U → V {\displaystyle f:U\to V} is bijective.

[1] Writing f = ( f 1 , … , f n ) {\displaystyle f=(f_{1},\ldots ,f_{n})} , this means that the system of n equations y i = f i ( x 1 , … , x n ) {\displaystyle y_{i}=f_{i}(x_{1},\dots ,x_{n})} has a unique solution for x 1 , … , x n {\displaystyle x_{1},\dots ,x_{n}} in terms of y 1 , … , y n {\displaystyle y_{1},\dots ,y_{n}} when x ∈ U , y ∈ V {\displaystyle x\in U,y\in V}.

Note that the theorem does not say f {\displaystyle f} is bijective onto the image where f ′ {\displaystyle f'} is invertible but that it is locally bijective where f ′ {\displaystyle f'} is invertible.

Moreover, the theorem says that the inverse function f − 1 : V → U {\displaystyle f^{-1}:V\to U} is continuously differentiable, and its derivative at b = f ( a ) {\displaystyle b=f(a)} is the inverse map of f ′ ( a ) {\displaystyle f'(a)} ; i.

e.

, ( f − 1 ) ′ ( b ) = f ′ ( a ) − 1.

{\displaystyle (f^{-1})'(b)=f'(a)^{-1}.

} In other words, if J f − 1 ( b ) , J f ( a ) {\displaystyle Jf^{-1}(b),Jf(a)} are the Jacobian matrices representing ( f − 1 ) ′ ( b ) , f ′ ( a ) {\displaystyle (f^{-1})'(b),f'(a)} , this means: J f − 1 ( b ) = J f ( a ) − 1.

{\displaystyle Jf^{-1}(b)=Jf(a)^{-1}.

} The hard part of the theorem is the existence and differentiability of f − 1 {\displaystyle f^{-1}}.

Assuming this, the inverse derivative formula follows from the chain rule applied to f − 1 ∘ f = I {\displaystyle f^{-1}\circ f=I}.

(Indeed, 1 = I ′ ( a ) = ( f − 1 ∘ f ) ′ ( a ) = ( f − 1 ) ′ ( b ) ∘ f ′ ( a ).

{\displaystyle 1=I'(a)=(f^{-1}\circ f)'(a)=(f^{-1})'(b)\circ f'(a).

} ) Since taking the inverse is infinitely differentiable, the formula for the derivative of the inverse shows that if f {\displaystyle f} is continuously k {\displaystyle k} times differentiable, with invertible derivative at the point a , then the inverse is also continuously k {\displaystyle k} times differentiable.

Here k {\displaystyle k} is a positive integer or ∞ {\displaystyle \infty }.

There are two variants of the inverse function theorem.

[1] Given a continuously differentiable map f : U → R m {\displaystyle f:U\to \mathbb {R} ^{m}} , the first is The derivative f ′ ( a ) {\displaystyle f'(a)} is surjective (i.

e.

, the Jacobian matrix representing it has rank m {\displaystyle m} ) if and only if there exists a continuously differentiable function g {\displaystyle g} on a neighborhood V {\displaystyle V} of b = f ( a ) {\displaystyle b=f(a)} such f ∘ g = I {\displaystyle f\circ g=I} near b {\displaystyle b} , and the second is The derivative f ′ ( a ) {\displaystyle f'(a)} is injective if and only if there exists a continuously differentiable function g {\displaystyle g} on a neighborhood V {\displaystyle V} of b = f ( a ) {\displaystyle b=f(a)} such g ∘ f = I {\displaystyle g\circ f=I} near a {\displaystyle a}.

In the first case (when f ′ ( a ) {\displaystyle f'(a)} is surjective), the point b = f ( a ) {\displaystyle b=f(a)} is called a regular value.

Since m = dim ⁡ ker ⁡ ( f ′ ( a ) ) + dim ⁡ im ⁡ ( f ′ ( a ) ) {\displaystyle m=\dim \ker(f'(a))+\dim \operatorname {im} (f'(a))} , the first case is equivalent to saying b = f ( a ) {\displaystyle b=f(a)} is not in the image of critical points a {\displaystyle a} (a critical point is a point a {\displaystyle a} such that the kernel of f ′ ( a ) {\displaystyle f'(a)} is nonzero).

The statement in the first case is a special case of the submersion theorem.

These variants are restatements of the inverse functions theorem.

Indeed, in the first case when f ′ ( a ) {\displaystyle f'(a)} is surjective, we can find an (injective) linear map T {\displaystyle T} such that f ′ ( a ) ∘ T = I {\displaystyle f'(a)\circ T=I}.

Define h ( x ) = a + T x {\displaystyle h(x)=a+Tx} so that we have: ( f ∘ h ) ′ ( 0 ) = f ′ ( a ) ∘ T = I.

{\displaystyle (f\circ h)'(0)=f'(a)\circ T=I.

} Thus, by the inverse function theorem, f ∘ h {\displaystyle f\circ h} has inverse near 0 {\displaystyle 0} ; i.

e.

, f ∘ h ∘ ( f ∘ h ) − 1 = I {\displaystyle f\circ h\circ (f\circ h)^{-1}=I} near b {\displaystyle b}.

The second case ( f ′ ( a ) {\displaystyle f'(a)} is injective) is seen in the similar way.

Example [ edit ] Consider the vector-valued function F : R 2 → R 2 {\displaystyle F:\mathbb {R} ^{2}\to \mathbb {R} ^{2}\!} defined by: F ( x , y ) = [ e x cos ⁡ y e x sin ⁡ y ].

{\displaystyle F(x,y)={\begin{bmatrix}{e^{x}\cos y}\\{e^{x}\sin y}\\\end{bmatrix}}.

} The Jacobian matrix is: J F ( x , y ) = [ e x cos ⁡ y − e x sin ⁡ y e x sin ⁡ y e x cos ⁡ y ] {\displaystyle J_{F}(x,y)={\begin{bmatrix}{e^{x}\cos y}&{-e^{x}\sin y}\\{e^{x}\sin y}&{e^{x}\cos y}\\\end{bmatrix}}} with Jacobian determinant: det J F ( x , y ) = e 2 x cos 2 ⁡ y + e 2 x sin 2 ⁡ y = e 2 x.

{\displaystyle \det J_{F}(x,y)=e^{2x}\cos ^{2}y+e^{2x}\sin ^{2}y=e^{2x}.

\,\!} The determinant e 2 x {\displaystyle e^{2x}\!} is nonzero everywhere.

Thus the theorem guarantees that, for every point p in R 2 {\displaystyle \mathbb {R} ^{2}\!} , there exists a neighborhood about p over which F is invertible.

This does not mean F is invertible over its entire domain: in this case F is not even injective since it is periodic: F ( x , y ) = F ( x , y + 2 π ) {\displaystyle F(x,y)=F(x,y+2\pi )\!}.

Counter-example [ edit ] The function f ( x ) = x + 2 x 2 sin ⁡ ( 1 x ) {\displaystyle f(x)=x+2x^{2}\sin({\tfrac {1}{x}})} is bounded inside a quadratic envelope near the line y = x {\displaystyle y=x} , so f ′ ( 0 ) = 1 {\displaystyle f'(0)=1}.

Nevertheless, it has local max/min points accumulating at x = 0 {\displaystyle x=0} , so it is not one-to-one on any surrounding interval.

If one drops the assumption that the derivative is continuous, the function no longer need be invertible.

For example f ( x ) = x + 2 x 2 sin ⁡ ( 1 x ) {\displaystyle f(x)=x+2x^{2}\sin({\tfrac {1}{x}})} and f ( 0 ) = 0 {\displaystyle f(0)=0} has discontinuous derivative f ′ ( x ) = 1 − 2 cos ⁡ ( 1 x ) + 4 x sin ⁡ ( 1 x ) {\displaystyle f'\!(x)=1-2\cos({\tfrac {1}{x}})+4x\sin({\tfrac {1}{x}})} and f ′ ( 0 ) = 1 {\displaystyle f'\!(0)=1} , which vanishes arbitrarily close to x = 0 {\displaystyle x=0}.

These critical points are local max/min points of f {\displaystyle f} , so f {\displaystyle f} is not one-to-one (and not invertible) on any interval containing x = 0 {\displaystyle x=0}.

Intuitively, the slope f ′ ( 0 ) = 1 {\displaystyle f'\!(0)=1} does not propagate to nearby points, where the slopes are governed by a weak but rapid oscillation.

Methods of proof [ edit ] As an important result, the inverse function theorem has been given numerous proofs.

The proof most commonly seen in textbooks relies on the contraction mapping principle, also known as the Banach fixed-point theorem (which can also be used as the key step in the proof of existence and uniqueness of solutions to ordinary differential equations ).

[2] [3] Since the fixed point theorem applies in infinite-dimensional (Banach space) settings, this proof generalizes immediately to the infinite-dimensional version of the inverse function theorem [4] (see Generalizations below).

An alternate proof in finite dimensions hinges on the extreme value theorem for functions on a compact set.

[5] Yet another proof uses Newton's method , which has the advantage of providing an effective version of the theorem: bounds on the derivative of the function imply an estimate of the size of the neighborhood on which the function is invertible.

[6] A proof using successive approximation [ edit ] To prove existence, it can be assumed after an affine transformation that f ( 0 ) = 0 {\displaystyle f(0)=0} and f ′ ( 0 ) = I {\displaystyle f^{\prime }(0)=I} , so that a = b = 0 {\displaystyle a=b=0}.

By the mean value theorem for vector-valued functions , for a function u : [ 0 , 1 ] → R m {\displaystyle u:[0,1]\to \mathbb {R} ^{m}} , ‖ u ( 1 ) − u ( 0 ) ‖ ≤ sup 0 ≤ t ≤ 1 ‖ u ′ ( t ) ‖ {\textstyle \|u(1)-u(0)\|\leq \sup _{0\leq t\leq 1}\|u^{\prime }(t)\|}.

Setting u ( t ) = f ( x + t ( x ′ − x ) ) − x − t ( x ′ − x ) {\displaystyle u(t)=f(x+t(x^{\prime }-x))-x-t(x^{\prime }-x)} , it follows that ‖ f ( x ) − f ( x ′ ) − x + x ′ ‖ ≤ ‖ x − x ′ ‖ sup 0 ≤ t ≤ 1 ‖ f ′ ( x + t ( x ′ − x ) ) − I ‖.

{\displaystyle \|f(x)-f(x^{\prime })-x+x^{\prime }\|\leq \|x-x^{\prime }\|\,\sup _{0\leq t\leq 1}\|f^{\prime }(x+t(x^{\prime }-x))-I\|.

} Now choose δ > 0 {\displaystyle \delta >0} so that ‖ f ′ ( x ) − I ‖ < 1 2 {\textstyle \|f'(x)-I\|<{1 \over 2}} for ‖ x ‖ < δ {\displaystyle \|x\|<\delta }.

Suppose that ‖ y ‖ < δ / 2 {\displaystyle \|y\|<\delta /2} and define x n {\displaystyle x_{n}} inductively by x 0 = 0 {\displaystyle x_{0}=0} and x n + 1 = x n + y − f ( x n ) {\displaystyle x_{n+1}=x_{n}+y-f(x_{n})}.

The assumptions show that if ‖ x ‖ , ‖ x ′ ‖ < δ {\displaystyle \|x\|,\,\,\|x^{\prime }\|<\delta } then ‖ f ( x ) − f ( x ′ ) − x + x ′ ‖ ≤ ‖ x − x ′ ‖ / 2 {\displaystyle \|f(x)-f(x^{\prime })-x+x^{\prime }\|\leq \|x-x^{\prime }\|/2}.

In particular f ( x ) = f ( x ′ ) {\displaystyle f(x)=f(x^{\prime })} implies x = x ′ {\displaystyle x=x^{\prime }}.

In the inductive scheme ‖ x n ‖ < δ {\displaystyle \|x_{n}\|<\delta } and ‖ x n + 1 − x n ‖ < δ / 2 n {\displaystyle \|x_{n+1}-x_{n}\|<\delta /2^{n}}.

Thus ( x n ) {\displaystyle (x_{n})} is a Cauchy sequence tending to x {\displaystyle x}.

By construction f ( x ) = y {\displaystyle f(x)=y} as required.

To check that g = f − 1 {\displaystyle g=f^{-1}} is C 1 , write g ( y + k ) = x + h {\displaystyle g(y+k)=x+h} so that f ( x + h ) = f ( x ) + k {\displaystyle f(x+h)=f(x)+k}.

By the inequalities above, ‖ h − k ‖ < ‖ h ‖ / 2 {\displaystyle \|h-k\|<\|h\|/2} so that ‖ h ‖ / 2 < ‖ k ‖ < 2 ‖ h ‖ {\displaystyle \|h\|/2<\|k\|<2\|h\|}.

On the other hand if A = f ′ ( x ) {\displaystyle A=f^{\prime }(x)} , then ‖ A − I ‖ < 1 / 2 {\displaystyle \|A-I\|<1/2}.

Using the geometric series for B = I − A {\displaystyle B=I-A} , it follows that ‖ A − 1 ‖ < 2 {\displaystyle \|A^{-1}\|<2}.

But then ‖ g ( y + k ) − g ( y ) − f ′ ( g ( y ) ) − 1 k ‖ ‖ k ‖ = ‖ h − f ′ ( x ) − 1 [ f ( x + h ) − f ( x ) ] ‖ ‖ k ‖ ≤ 4 ‖ f ( x + h ) − f ( x ) − f ′ ( x ) h ‖ ‖ h ‖ {\displaystyle {\|g(y+k)-g(y)-f^{\prime }(g(y))^{-1}k\| \over \|k\|}={\|h-f^{\prime }(x)^{-1}[f(x+h)-f(x)]\| \over \|k\|}\leq 4{\|f(x+h)-f(x)-f^{\prime }(x)h\| \over \|h\|}} tends to 0 as k {\displaystyle k} and h {\displaystyle h} tend to 0, proving that g {\displaystyle g} is C 1 with g ′ ( y ) = f ′ ( g ( y ) ) − 1 {\displaystyle g^{\prime }(y)=f^{\prime }(g(y))^{-1}}.

The proof above is presented for a finite-dimensional space, but applies equally well for Banach spaces.

If an invertible function f {\displaystyle f} is C k with k > 1 {\displaystyle k>1} , then so too is its inverse.

This follows by induction using the fact that the map F ( A ) = A − 1 {\displaystyle F(A)=A^{-1}} on operators is C k for any k {\displaystyle k} (in the finite-dimensional case this is an elementary fact because the inverse of a matrix is given as the adjugate matrix divided by its determinant ).

[1] [7] The method of proof here can be found in the books of Henri Cartan , Jean Dieudonné , Serge Lang , Roger Godement and Lars Hörmander.

A proof using the contraction mapping principle [ edit ] Here is a proof based on the contraction mapping theorem.

Specifically, following T.

Tao, [8] it uses the following consequence of the contraction mapping theorem.

Lemma — Let B ( 0 , r ) {\displaystyle B(0,r)} denote an open ball of radius r in R n {\displaystyle \mathbb {R} ^{n}} with center 0.

If g : B ( 0 , r ) → R n {\displaystyle g:B(0,r)\to \mathbb {R} ^{n}} is a map such that g ( 0 ) = 0 {\displaystyle g(0)=0} and there exists a constant 0 < c < 1 {\displaystyle 0<c<1} such that | g ( y ) − g ( x ) | ≤ c | y − x | {\displaystyle |g(y)-g(x)|\leq c|y-x|} for all x , y {\displaystyle x,y} in B ( 0 , r ) {\displaystyle B(0,r)} , then f = I + g {\displaystyle f=I+g} is injective on B ( 0 , r ) {\displaystyle B(0,r)} and B ( 0 , ( 1 − c ) r ) ⊂ f ( B ( 0 , r ) ) ⊂ B ( 0 , ( 1 + c ) r ) {\displaystyle B(0,(1-c)r)\subset f(B(0,r))\subset B(0,(1+c)r)}.

(More generally, the statement remains true if R n {\displaystyle \mathbb {R} ^{n}} is replaced by a Banach space.

) Basically, the lemma says that a small perturbation of the identity map by a contraction map is injective and preserves a ball in some sense.

Assuming the lemma for a moment, we prove the theorem first.

As in the above proof, it is enough to prove the special case when a = 0 , b = f ( a ) = 0 {\displaystyle a=0,b=f(a)=0} and f ′ ( 0 ) = I {\displaystyle f'(0)=I}.

Let g = f − I {\displaystyle g=f-I}.

The mean value inequality applied to t ↦ g ( x + t ( y − x ) ) {\displaystyle t\mapsto g(x+t(y-x))} says: | g ( y ) − g ( x ) | ≤ | y − x | sup 0 < t < 1 | g ′ ( x + t ( y − x ) ) |.

{\displaystyle |g(y)-g(x)|\leq |y-x|\sup _{0<t<1}|g'(x+t(y-x))|.

} Since g ′ ( 0 ) = I − I = 0 {\displaystyle g'(0)=I-I=0} and g ′ {\displaystyle g'} is continuous, we can find an r > 0 {\displaystyle r>0} such that | g ( y ) − g ( x ) | ≤ 2 − 1 | y − x | {\displaystyle |g(y)-g(x)|\leq 2^{-1}|y-x|} for all x , y {\displaystyle x,y} in B ( 0 , r ) {\displaystyle B(0,r)}.

Then the early lemma says that f = g + I {\displaystyle f=g+I} is injective on B ( 0 , r ) {\displaystyle B(0,r)} and B ( 0 , r / 2 ) ⊂ f ( B ( 0 , r ) ) {\displaystyle B(0,r/2)\subset f(B(0,r))}.

Then f : U = B ( 0 , r ) ∩ f − 1 ( B ( 0 , r / 2 ) ) → V = B ( 0 , r / 2 ) {\displaystyle f:U=B(0,r)\cap f^{-1}(B(0,r/2))\to V=B(0,r/2)} is bijective and thus has an inverse.

Next, we show the inverse f − 1 {\displaystyle f^{-1}} is continuously differentiable (this part of the argument is the same as that in the previous proof).

This time, let g = f − 1 {\displaystyle g=f^{-1}} denote the inverse of f {\displaystyle f} and A = f ′ ( x ) {\displaystyle A=f'(x)}.

For x = g ( y ) {\displaystyle x=g(y)} , we write g ( y + k ) = x + h {\displaystyle g(y+k)=x+h} or y + k = f ( x + h ) {\displaystyle y+k=f(x+h)}.

Now, by the early estimate, we have | h − k | = | f ( x + h ) − f ( x ) − h | ≤ | h | / 2 {\displaystyle |h-k|=|f(x+h)-f(x)-h|\leq |h|/2} and so | h | / 2 ≤ | k | {\displaystyle |h|/2\leq |k|}.

Writing ‖ ⋅ ‖ {\displaystyle \|\cdot \|} for the operator norm, | g ( y + k ) − g ( y ) − A − 1 k | = | h − A − 1 ( f ( x + h ) − f ( x ) ) | ≤ ‖ A − 1 ‖ | A h − f ( x + h ) + f ( x ) |.

{\displaystyle |g(y+k)-g(y)-A^{-1}k|=|h-A^{-1}(f(x+h)-f(x))|\leq \|A^{-1}\||Ah-f(x+h)+f(x)|.

} As k → 0 {\displaystyle k\to 0} , we have h → 0 {\displaystyle h\to 0} and | h | / | k | {\displaystyle |h|/|k|} is bounded.

Hence, g {\displaystyle g} is differentiable at y {\displaystyle y} with the derivative g ′ ( y ) = f ′ ( g ( y ) ) − 1 {\displaystyle g'(y)=f'(g(y))^{-1}}.

Also, g ′ {\displaystyle g'} is the same as the composition ι ∘ f ′ ∘ g {\displaystyle \iota \circ f'\circ g} where ι : T ↦ T − 1 {\displaystyle \iota :T\mapsto T^{-1}} ; so g ′ {\displaystyle g'} is continuous.

It remains to show the lemma.

First, the map f {\displaystyle f} is injective on B ( 0 , r ) {\displaystyle B(0,r)} since if f ( x ) = f ( y ) {\displaystyle f(x)=f(y)} , then g ( y ) − g ( x ) = x − y {\displaystyle g(y)-g(x)=x-y} and so | g ( y ) − g ( x ) | = | y − x | {\displaystyle |g(y)-g(x)|=|y-x|} , which is a contradiction unless y = x {\displaystyle y=x}.

(This part does not need the assumption g ( 0 ) = 0 {\displaystyle g(0)=0}.

) Next we show f ( B ( 0 , r ) ) ⊃ B ( 0 , ( 1 − c ) r ) {\displaystyle f(B(0,r))\supset B(0,(1-c)r)}.

The idea is to note that this is equivalent to, given a point y {\displaystyle y} in B ( 0 , ( 1 − c ) r ) {\displaystyle B(0,(1-c)r)} , find a fixed point of the map F : B ¯ ( 0 , r ′ ) → B ¯ ( 0 , r ′ ) , x ↦ y − g ( x ) {\displaystyle F:{\overline {B}}(0,r')\to {\overline {B}}(0,r'),\,x\mapsto y-g(x)} where 0 < r ′ < r {\displaystyle 0<r'<r} such that | y | ≤ ( 1 − c ) r ′ {\displaystyle |y|\leq (1-c)r'} and the bar means a closed ball.

To find a fixed point, we use the contraction mapping theorem and checking that F {\displaystyle F} is a well-defined strict-contraction mapping is straightforward.

Finally, we have: f ( B ( 0 , r ) ) ⊂ B ( 0 , ( 1 + c ) r ) {\displaystyle f(B(0,r))\subset B(0,(1+c)r)} since | f ( x ) | = | x + g ( x ) − g ( 0 ) | ≤ ( 1 + c ) | x |.

◻ {\displaystyle |f(x)|=|x+g(x)-g(0)|\leq (1+c)|x|.

\square } As might be clear, this proof is not substantially different from the previous one, as the proof of the contraction mapping theorem is by successive approximation.

Applications [ edit ] Implicit function theorem [ edit ] The inverse function theorem can be used to solve a system of equations f 1 ( x ) = y 1 ⋮ f n ( x ) = y n , {\displaystyle {\begin{aligned}&f_{1}(x)=y_{1}\\&\quad \vdots \\&f_{n}(x)=y_{n},\end{aligned}}} i.

e.

, expressing y 1 , … , y n {\displaystyle y_{1},\dots ,y_{n}} as functions of x = ( x 1 , … , x n ) {\displaystyle x=(x_{1},\dots ,x_{n})} , provided the Jacobian matrix is invertible.

The implicit function theorem allows to solve a more general system of equations: f 1 ( x , y ) = 0 ⋮ f n ( x , y ) = 0 {\displaystyle {\begin{aligned}&f_{1}(x,y)=0\\&\quad \vdots \\&f_{n}(x,y)=0\end{aligned}}} for y {\displaystyle y} in terms of x {\displaystyle x}.

Though more general, the theorem is actually a consequence of the inverse function theorem.

First, the precise statement of the implicit function theorem is as follows: [9] given a map f : R n × R m → R m {\displaystyle f:\mathbb {R} ^{n}\times \mathbb {R} ^{m}\to \mathbb {R} ^{m}} , if f ( a , b ) = 0 {\displaystyle f(a,b)=0} , f {\displaystyle f} is continuously differentiable in a neighborhood of ( a , b ) {\displaystyle (a,b)} and the derivative of y ↦ f ( a , y ) {\displaystyle y\mapsto f(a,y)} at b {\displaystyle b} is invertible, then there exists a differentiable map g : U → V {\displaystyle g:U\to V} for some neighborhoods U , V {\displaystyle U,V} of a , b {\displaystyle a,b} such that f ( x , g ( x ) ) = 0 {\displaystyle f(x,g(x))=0}.

Moreover, if f ( x , y ) = 0 , x ∈ U , y ∈ V {\displaystyle f(x,y)=0,x\in U,y\in V} , then y = g ( x ) {\displaystyle y=g(x)} ; i.

e.

, g ( x ) {\displaystyle g(x)} is a unique solution.

To see this, consider the map F ( x , y ) = ( x , f ( x , y ) ) {\displaystyle F(x,y)=(x,f(x,y))}.

By the inverse function theorem, F : U × V → W {\displaystyle F:U\times V\to W} has the inverse G {\displaystyle G} for some neighborhoods U , V , W {\displaystyle U,V,W}.

We then have: ( x , y ) = F ( G 1 ( x , y ) , G 2 ( x , y ) ) = ( G 1 ( x , y ) , f ( G 1 ( x , y ) , G 2 ( x , y ) ) , {\displaystyle (x,y)=F(G_{1}(x,y),G_{2}(x,y))=(G_{1}(x,y),f(G_{1}(x,y),G_{2}(x,y)),} implying x = G 1 ( x , y ) {\displaystyle x=G_{1}(x,y)} and y = f ( x , G 2 ( x , y ) ).

{\displaystyle y=f(x,G_{2}(x,y)).

} Thus g ( x ) = G 2 ( x , 0 ) {\displaystyle g(x)=G_{2}(x,0)} has the required property.

◻ {\displaystyle \square } Giving a manifold structure [ edit ] In differential geometry, the inverse function theorem is used to show that the pre-image of a regular value under a smooth map is a manifold.

[10] Indeed, let f : U → R r {\displaystyle f:U\to \mathbb {R} ^{r}} be such a smooth map from an open subset of R n {\displaystyle \mathbb {R} ^{n}} (since the result is local, there is no loss of generality with considering such a map).

Fix a point a {\displaystyle a} in f − 1 ( b ) {\displaystyle f^{-1}(b)} and then, by permuting the coordinates on R n {\displaystyle \mathbb {R} ^{n}} , assume the matrix [ ∂ f i ∂ x j ( a ) ] 1 ≤ i , j ≤ r {\displaystyle \left[{\frac {\partial f_{i}}{\partial x_{j}}}(a)\right]_{1\leq i,j\leq r}} has rank r {\displaystyle r}.

Then the map F : U → R r × R n − r = R n , x ↦ ( f ( x ) , x r + 1 , … , x n ) {\displaystyle F:U\to \mathbb {R} ^{r}\times \mathbb {R} ^{n-r}=\mathbb {R} ^{n},\,x\mapsto (f(x),x_{r+1},\dots ,x_{n})} is such that F ′ ( a ) {\displaystyle F'(a)} has rank n {\displaystyle n}.

Hence, by the inverse function theorem, we find the smooth inverse G {\displaystyle G} of F {\displaystyle F} defined in a neighborhood V × W {\displaystyle V\times W} of ( b , a r + 1 , … , a n ) {\displaystyle (b,a_{r+1},\dots ,a_{n})}.

We then have x = ( F ∘ G ) ( x ) = ( f ( G ( x ) ) , G r + 1 ( x ) , … , G n ( x ) ) , {\displaystyle x=(F\circ G)(x)=(f(G(x)),G_{r+1}(x),\dots ,G_{n}(x)),} which implies ( f ∘ G ) ( x 1 , … , x n ) = ( x 1 , … , x r ).

{\displaystyle (f\circ G)(x_{1},\dots ,x_{n})=(x_{1},\dots ,x_{r}).

} That is, after the change of coordinates by G {\displaystyle G} , f {\displaystyle f} is a coordinate projection (this fact is known as the submersion theorem ).

Moreover, since G : V × W → U ′ = G ( V × W ) {\displaystyle G:V\times W\to U'=G(V\times W)} is bijective, the map g = G ( b , ⋅ ) : W → f − 1 ( b ) ∩ U ′ , ( x r + 1 , … , x n ) ↦ G ( b , x r + 1 , … , x n ) {\displaystyle g=G(b,\cdot ):W\to f^{-1}(b)\cap U',\,(x_{r+1},\dots ,x_{n})\mapsto G(b,x_{r+1},\dots ,x_{n})} is bijective with the smooth inverse.

That is to say, g {\displaystyle g} gives a local parametrization of f − 1 ( b ) {\displaystyle f^{-1}(b)} around a {\displaystyle a}.

Hence, f − 1 ( b ) {\displaystyle f^{-1}(b)} is a manifold.

◻ {\displaystyle \square } (Note the proof is quite similar to the proof of the implicit function theorem and, in fact, the implicit function theorem can be also used instead.

) More generally, the theorem shows that if a smooth map f : P → E {\displaystyle f:P\to E} is transversal to a submanifold M ⊂ E {\displaystyle M\subset E} , then the pre-image f − 1 ( M ) ↪ P {\displaystyle f^{-1}(M)\hookrightarrow P} is a submanifold.

[11] Global version [ edit ] The inverse function theorem is a local result; it applies to each point.

A priori , the theorem thus only shows the function f {\displaystyle f} is locally bijective (or locally diffeomorphic of some class).

The next topological lemma can be used to upgrade local injectivity to injectivity that is global to some extent.

Lemma — [12] [ full citation needed ] [13] If A {\displaystyle A} is a closed subset of a (second-countable) topological manifold X {\displaystyle X} (or, more generally, a topological space admitting an exhaustion by compact subsets ) and f : X → Z {\displaystyle f:X\to Z} , Z {\displaystyle Z} some topological space, is a local homeomorphism that is injective on A {\displaystyle A} , then f {\displaystyle f} is injective on some neighborhood of A {\displaystyle A}.

Proof: [14] First assume X {\displaystyle X} is compact.

If the conclusion of the theorem is false, we can find two sequences x i ≠ y i {\displaystyle x_{i}\neq y_{i}} such that f ( x i ) = f ( y i ) {\displaystyle f(x_{i})=f(y_{i})} and x i , y i {\displaystyle x_{i},y_{i}} each converge to some points x , y {\displaystyle x,y} in A {\displaystyle A}.

Since f {\displaystyle f} is injective on A {\displaystyle A} , x = y {\displaystyle x=y}.

Now, if i {\displaystyle i} is large enough, x i , y i {\displaystyle x_{i},y_{i}} are in a neighborhood of x = y {\displaystyle x=y} where f {\displaystyle f} is injective; thus, x i = y i {\displaystyle x_{i}=y_{i}} , a contradiction.

In general, consider the set E = { ( x , y ) ∈ X 2 ∣ x ≠ y , f ( x ) = f ( y ) } {\displaystyle E=\{(x,y)\in X^{2}\mid x\neq y,f(x)=f(y)\}}.

It is disjoint from S × S {\displaystyle S\times S} for any subset S ⊂ X {\displaystyle S\subset X} where f {\displaystyle f} is injective.

Let X 1 ⊂ X 2 ⊂ ⋯ {\displaystyle X_{1}\subset X_{2}\subset \cdots } be an increasing sequence of compact subsets with union X {\displaystyle X} and with X i {\displaystyle X_{i}} contained in the interior of X i + 1 {\displaystyle X_{i+1}}.

Then, by the first part of the proof, for each i {\displaystyle i} , we can find a neighborhood U i {\displaystyle U_{i}} of A ∩ X i {\displaystyle A\cap X_{i}} such that U i 2 ⊂ X 2 − E {\displaystyle U_{i}^{2}\subset X^{2}-E}.

Then U = ⋃ i U i {\displaystyle U=\bigcup _{i}U_{i}} has the required property.

◻ {\displaystyle \square } (See also [15] for an alternative approach.

) The lemma implies the following (a sort of) global version of the inverse function theorem: Inverse function theorem — [16] Let f : U → V {\displaystyle f:U\to V} be a map between open subsets of R n , R m {\displaystyle \mathbb {R} ^{n},\mathbb {R} ^{m}} or more generally of manifolds.

Assume f {\displaystyle f} is continuously differentiable (or is C k {\displaystyle C^{k}} ).

If f {\displaystyle f} is injective on a closed subset A ⊂ U {\displaystyle A\subset U} and if the Jacobian matrix of f {\displaystyle f} is invertible at each point of A {\displaystyle A} , then f {\displaystyle f} is injective in a neighborhood A ′ {\displaystyle A'} of A {\displaystyle A} and f − 1 : f ( A ′ ) → A ′ {\displaystyle f^{-1}:f(A')\to A'} is continuously differentiable (or is C k {\displaystyle C^{k}} ).

Note that if A {\displaystyle A} is a point, then the above is the usual inverse function theorem.

Holomorphic inverse function theorem [ edit ] There is a version of the inverse function theorem for holomorphic maps.

Theorem — [17] [18] Let U , V ⊂ C n {\displaystyle U,V\subset \mathbb {C} ^{n}} be open subsets such that 0 ∈ U {\displaystyle 0\in U} and f : U → V {\displaystyle f:U\to V} a holomorphic map whose Jacobian matrix in variables z i , z ¯ i {\displaystyle z_{i},{\overline {z}}_{i}} is invertible (the determinant is nonzero) at 0 {\displaystyle 0}.

Then f {\displaystyle f} is injective in some neighborhood W {\displaystyle W} of 0 {\displaystyle 0} and the inverse f − 1 : f ( W ) → W {\displaystyle f^{-1}:f(W)\to W} is holomorphic.

The theorem follows from the usual inverse function theorem.

Indeed, let J R ( f ) {\displaystyle J_{\mathbb {R} }(f)} denote the Jacobian matrix of f {\displaystyle f} in variables x i , y i {\displaystyle x_{i},y_{i}} and J ( f ) {\displaystyle J(f)} for that in z j , z ¯ j {\displaystyle z_{j},{\overline {z}}_{j}}.

Then we have det J R ( f ) = | det J ( f ) | 2 {\displaystyle \det J_{\mathbb {R} }(f)=|\det J(f)|^{2}} , which is nonzero by assumption.

Hence, by the usual inverse function theorem, f {\displaystyle f} is injective near 0 {\displaystyle 0} with continuously differentiable inverse.

By chain rule, with w = f ( z ) {\displaystyle w=f(z)} , ∂ ∂ z ¯ j ( f j − 1 ∘ f ) ( z ) = ∑ k ∂ f j − 1 ∂ w k ( w ) ∂ f k ∂ z ¯ j ( z ) + ∑ k ∂ f j − 1 ∂ w ¯ k ( w ) ∂ f ¯ k ∂ z ¯ j ( z ) {\displaystyle {\frac {\partial }{\partial {\overline {z}}_{j}}}(f_{j}^{-1}\circ f)(z)=\sum _{k}{\frac {\partial f_{j}^{-1}}{\partial w_{k}}}(w){\frac {\partial f_{k}}{\partial {\overline {z}}_{j}}}(z)+\sum _{k}{\frac {\partial f_{j}^{-1}}{\partial {\overline {w}}_{k}}}(w){\frac {\partial {\overline {f}}_{k}}{\partial {\overline {z}}_{j}}}(z)} where the left-hand side and the first term on the right vanish since f j − 1 ∘ f {\displaystyle f_{j}^{-1}\circ f} and f k {\displaystyle f_{k}} are holomorphic.

Thus, ∂ f j − 1 ∂ w ¯ k ( w ) = 0 {\displaystyle {\frac {\partial f_{j}^{-1}}{\partial {\overline {w}}_{k}}}(w)=0} for each k {\displaystyle k}.

◻ {\displaystyle \square } Similarly, there is the implicit function theorem for holomorphic functions.

[19] As already noted earlier, it can happen that an injective smooth function has the inverse that is not smooth (e.

g.

, f ( x ) = x 3 {\displaystyle f(x)=x^{3}} in a real variable).

This is not the case for holomorphic functions because of: Proposition — [19] If f : U → V {\displaystyle f:U\to V} is an injective holomorphic map between open subsets of C n {\displaystyle \mathbb {C} ^{n}} , then f − 1 : f ( U ) → U {\displaystyle f^{-1}:f(U)\to U} is holomorphic.

Formulations for manifolds [ edit ] The inverse function theorem can be rephrased in terms of differentiable maps between differentiable manifolds.

In this context the theorem states that for a differentiable map F : M → N {\displaystyle F:M\to N} (of class C 1 {\displaystyle C^{1}} ), if the differential of F {\displaystyle F} , d F p : T p M → T F ( p ) N {\displaystyle dF_{p}:T_{p}M\to T_{F(p)}N} is a linear isomorphism at a point p {\displaystyle p} in M {\displaystyle M} then there exists an open neighborhood U {\displaystyle U} of p {\displaystyle p} such that F | U : U → F ( U ) {\displaystyle F|_{U}:U\to F(U)} is a diffeomorphism.

Note that this implies that the connected components of M and N containing p and F ( p ) have the same dimension, as is already directly implied from the assumption that dF p is an isomorphism.

If the derivative of F is an isomorphism at all points p in M then the map F is a local diffeomorphism.

Generalizations [ edit ] Banach spaces [ edit ] The inverse function theorem can also be generalized to differentiable maps between Banach spaces X and Y.

[20] Let U be an open neighbourhood of the origin in X and F : U → Y {\displaystyle F:U\to Y\!} a continuously differentiable function, and assume that the Fréchet derivative d F 0 : X → Y {\displaystyle dF_{0}:X\to Y\!} of F at 0 is a bounded linear isomorphism of X onto Y.

Then there exists an open neighbourhood V of F ( 0 ) {\displaystyle F(0)\!} in Y and a continuously differentiable map G : V → X {\displaystyle G:V\to X\!} such that F ( G ( y ) ) = y {\displaystyle F(G(y))=y} for all y in V.

Moreover, G ( y ) {\displaystyle G(y)\!} is the only sufficiently small solution x of the equation F ( x ) = y {\displaystyle F(x)=y\!}.

There is also the inverse function theorem for Banach manifolds.

[21] Constant rank theorem [ edit ] The inverse function theorem (and the implicit function theorem ) can be seen as a special case of the constant rank theorem, which states that a smooth map with constant rank near a point can be put in a particular normal form near that point.

[22] Specifically, if F : M → N {\displaystyle F:M\to N} has constant rank near a point p ∈ M {\displaystyle p\in M\!} , then there are open neighborhoods U of p and V of F ( p ) {\displaystyle F(p)\!} and there are diffeomorphisms u : T p M → U {\displaystyle u:T_{p}M\to U\!} and v : T F ( p ) N → V {\displaystyle v:T_{F(p)}N\to V\!} such that F ( U ) ⊆ V {\displaystyle F(U)\subseteq V\!} and such that the derivative d F p : T p M → T F ( p ) N {\displaystyle dF_{p}:T_{p}M\to T_{F(p)}N\!} is equal to v − 1 ∘ F ∘ u {\displaystyle v^{-1}\circ F\circ u\!}.

That is, F "looks like" its derivative near p.

The set of points p ∈ M {\displaystyle p\in M} such that the rank is constant in a neighborhood of p {\displaystyle p} is an open dense subset of M ; this is a consequence of semicontinuity of the rank function.

Thus the constant rank theorem applies to a generic point of the domain.

When the derivative of F is injective (resp.

surjective) at a point p , it is also injective (resp.

surjective) in a neighborhood of p , and hence the rank of F is constant on that neighborhood, and the constant rank theorem applies.

Polynomial functions [ edit ] If it is true, the Jacobian conjecture would be a variant of the inverse function theorem for polynomials.

It states that if a vector-valued polynomial function has a Jacobian determinant that is an invertible polynomial (that is a nonzero constant), then it has an inverse that is also a polynomial function.

It is unknown whether this is true or false, even in the case of two variables.

This is a major open problem in the theory of polynomials.

Selections [ edit ] When f : R n → R m {\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} ^{m}} with m ≤ n {\displaystyle m\leq n} , f {\displaystyle f} is k {\displaystyle k} times continuously differentiable , and the Jacobian A = ∇ f ( x ¯ ) {\displaystyle A=\nabla f({\overline {x}})} at a point x ¯ {\displaystyle {\overline {x}}} is of rank m {\displaystyle m} , the inverse of f {\displaystyle f} may not be unique.

However, there exists a local selection function s {\displaystyle s} such that f ( s ( y ) ) = y {\displaystyle f(s(y))=y} for all y {\displaystyle y} in a neighborhood of y ¯ = f ( x ¯ ) {\displaystyle {\overline {y}}=f({\overline {x}})} , s ( y ¯ ) = x ¯ {\displaystyle s({\overline {y}})={\overline {x}}} , s {\displaystyle s} is k {\displaystyle k} times continuously differentiable in this neighborhood, and ∇ s ( y ¯ ) = A T ( A A T ) − 1 {\displaystyle \nabla s({\overline {y}})=A^{T}(AA^{T})^{-1}} ( ∇ s ( y ¯ ) {\displaystyle \nabla s({\overline {y}})} is the Moore–Penrose pseudoinverse of A {\displaystyle A} ).

[23] See also [ edit ] Nash–Moser theorem Notes [ edit ] ^ a b c Theorem 1.

1.

7.

in Hörmander, Lars (2015).

The Analysis of Linear Partial Differential Operators I: Distribution Theory and Fourier Analysis.

Classics in Mathematics (2nd ed.

).

Springer.

ISBN 978-3-642-61497-2.

^ McOwen, Robert C.

(1996).

"Calculus of Maps between Banach Spaces".

Partial Differential Equations: Methods and Applications.

Upper Saddle River, NJ: Prentice Hall.

pp.

218–224.

ISBN 0-13-121880-8.

^ Tao, Terence (12 September 2011).

"The inverse function theorem for everywhere differentiable maps".

Retrieved 26 July 2019.

^ Jaffe, Ethan.

"Inverse Function Theorem" (PDF).

^ Spivak 1965 , pages 31–35 ^ Hubbard, John H.

; Hubbard, Barbara Burke (2001).

Vector Analysis, Linear Algebra, and Differential Forms: A Unified Approach (Matrix ed.

).

^ Cartan, Henri (1971).

Calcul Differentiel (in French).

Hermann.

pp.

55–61.

ISBN 978-0-395-12033-0.

^ Theorem 17.

7.

2 in Tao, Terence (2014).

Analysis.

II.

Texts and Readings in Mathematics.

Vol.

38 (Third edition of 2006 original ed.

).

New Delhi: Hindustan Book Agency.

ISBN 978-93-80250-65-6.

MR 3310023.

Zbl 1300.

26003.

^ Spivak 1965 , Theorem 2-12.

^ Spivak 1965 , Theorem 5-1.

and Theorem 2-13.

^ "Transversality" (PDF).

northwestern.

edu.

^ One of Spivak's books (Editorial note: give the exact location).

^ Hirsch 1976 , Ch.

2, § 1.

, Exercise 7.

NB: This one is for a C 1 {\displaystyle C^{1}} -immersion.

^ Lemma 13.

3.

3.

of Lectures on differential topology utoronto.

ca ^ Dan Ramras ( https://mathoverflow.

net/users/4042/dan-ramras ), On a proof of the existence of tubular neighborhoods.

, URL (version: 2017-04-13): https://mathoverflow.

net/q/58124 ^ Ch.

I.

, § 3, Exercise 10.

and § 8, Exercise 14.

in V.

Guillemin, A.

Pollack.

"Differential Topology".

Prentice-Hall Inc.

, 1974.

ISBN 0-13-212605-2.

^ Griffiths & Harris 1978 , p.

18.

^ Fritzsche, K.

; Grauert, H.

(2002).

From Holomorphic Functions to Complex Manifolds.

Springer.

pp.

33–36.

ISBN 978-0-387-95395-3.

^ a b Griffiths & Harris 1978 , p.

19.

^ Luenberger, David G.

(1969).

Optimization by Vector Space Methods.

New York: John Wiley & Sons.

pp.

240–242.

ISBN 0-471-55359-X.

^ Lang, Serge (1985).

Differential Manifolds.

New York: Springer.

pp.

13–19.

ISBN 0-387-96113-5.

^ Boothby, William M.

(1986).

An Introduction to Differentiable Manifolds and Riemannian Geometry (Second ed.

).

Orlando: Academic Press.

pp.

46–50.

ISBN 0-12-116052-1.

^ Dontchev, Asen L.

; Rockafellar, R.

Tyrrell (2014).

Implicit Functions and Solution Mappings: A View from Variational Analysis (Second ed.

).

New York: Springer-Verlag.

p.

54.

ISBN 978-1-4939-1036-6.

References [ edit ] Allendoerfer, Carl B.

(1974).

"Theorems about Differentiable Functions".

Calculus of Several Variables and Differentiable Manifolds.

New York: Macmillan.

pp.

54–88.

ISBN 0-02-301840-2.

Baxandall, Peter ; Liebeck, Hans (1986).

"The Inverse Function Theorem".

Vector Calculus.

New York: Oxford University Press.

pp.

214–225.

ISBN 0-19-859652-9.

Nijenhuis, Albert (1974).

"Strong derivatives and inverse mappings".

Amer.

Math.

Monthly.

81 (9): 969–980.

doi : 10.

2307/2319298.

hdl : 10338.

dmlcz/102482.

JSTOR 2319298.

Griffiths, Phillip; Harris, Joseph (1978), Principles of Algebraic Geometry , John Wiley & Sons, ISBN 978-0-471-05059-9.

Hirsch, Morris W.

(1976).

Differential Topology.

Springer-Verlag.

ISBN 978-0-387-90148-0.

Protter, Murray H.

; Morrey, Charles B.

Jr.

(1985).

"Transformations and Jacobians".

Intermediate Calculus (Second ed.

).

New York: Springer.

pp.

412–420.

ISBN 0-387-96058-9.

Renardy, Michael; Rogers, Robert C.

(2004).

An Introduction to Partial Differential Equations.

Texts in Applied Mathematics 13 (Second ed.

).

New York: Springer-Verlag.

pp.

337–338.

ISBN 0-387-00444-0.

Rudin, Walter (1976).

Principles of mathematical analysis.

International Series in Pure and Applied Mathematics (Third ed.

).

New York: McGraw-Hill Book.

pp.

221 –223.

ISBN 978-0-07-085613-4.

Spivak, Michael (1965).

Calculus on Manifolds: A Modern Approach to Classical Theorems of Advanced Calculus.

San Francisco: Benjamin Cummings.

ISBN 0-8053-9021-9.

v t e Functional analysis ( topics – glossary ) Spaces Banach Besov Fréchet Hilbert Hölder Nuclear Orlicz Schwartz Sobolev Topological vector Properties Barrelled Complete Dual ( Algebraic / Topological ) Locally convex Reflexive Separable Theorems Hahn–Banach Riesz representation Closed graph Uniform boundedness principle Kakutani fixed-point Krein–Milman Min–max Gelfand–Naimark Banach–Alaoglu Operators Adjoint Bounded Compact Hilbert–Schmidt Normal Nuclear Trace class Transpose Unbounded Unitary Algebras Banach algebra C*-algebra Spectrum of a C*-algebra Operator algebra Group algebra of a locally compact group Von Neumann algebra Open problems Invariant subspace problem Mahler's conjecture Applications Hardy space Spectral theory of ordinary differential equations Heat kernel Index theorem Calculus of variations Functional calculus Integral operator Jones polynomial Topological quantum field theory Noncommutative geometry Riemann hypothesis Distribution (or Generalized functions ) Advanced topics Approximation property Balanced set Choquet theory Weak topology Banach–Mazur distance Tomita–Takesaki theory Category v t e Analysis in topological vector spaces Basic concepts Abstract Wiener space Classical Wiener space Bochner space Convex series Cylinder set measure Infinite-dimensional vector function Matrix calculus Vector calculus Derivatives Differentiable vector–valued functions from Euclidean space Differentiation in Fréchet spaces Fréchet derivative Total Functional derivative Gateaux derivative Directional Generalizations of the derivative Hadamard derivative Holomorphic Quasi-derivative Measurability Besov measure Cylinder set measure Canonical Gaussian Classical Wiener measure Measure like set functions infinite-dimensional Gaussian measure Projection-valued Vector Bochner / Weakly / Strongly measurable function Radonifying function Integrals Bochner Direct integral Dunford Gelfand–Pettis/Weak Regulated Paley–Wiener Results Cameron–Martin theorem Inverse function theorem Nash–Moser theorem Feldman–Hájek theorem No infinite-dimensional Lebesgue measure Sazonov's theorem Structure theorem for Gaussian measures Related Crinkled arc Covariance operator Functional calculus Borel functional calculus Continuous functional calculus Holomorphic functional calculus Applications Banach manifold ( bundle ) Convenient vector space Choquet theory Fréchet manifold Hilbert manifold Retrieved from " https://en.

wikipedia.

org/w/index.

php?title=Inverse_function_theorem&oldid=1217673429 " Categories : Multivariable calculus Differential topology Inverse functions Theorems in real analysis Theorems in calculus Hidden categories: CS1 French-language sources (fr) Articles with short description Short description is different from Wikidata Use dmy dates from December 2023 Pages using sidebar with the child parameter All articles with incomplete citations Articles with incomplete citations from August 2023 This page was last edited on 7 April 2024, at 05:50 (UTC).

Text is available under the Creative Commons Attribution-ShareAlike License 4.

0 ; additional terms may apply.

By using this site, you agree to the Terms of Use and Privacy Policy.

Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc.

, a non-profit organization.

Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view.